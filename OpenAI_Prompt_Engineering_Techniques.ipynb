{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-for-Professionals/blob/main/OpenAI_Prompt_Engineering_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rn1lLJXRfSZ"
      },
      "source": [
        "# Prompt Engineering Techniques\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "1dXZcBM5xAiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhdJw2HoxALo",
        "outputId": "0ba5369f-1b5b-4246-de93-35b1a9143fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.8/120.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use OpenAI GPT model (gpt-3.5-turbo) for these given below prompting techniques\n",
        "\n",
        "### There are different prompting techniques:\n",
        "**1) Multi-turn conversation or Chatbot with conversation history** <br>\n",
        "**2) Zero-shot Prompting** <br>\n",
        "**3) Few-shot Prompting** <br>\n",
        "**4) Chain-of-Thought Prompting** <br>\n",
        "**5) Tree-of-Thought Prompting** <br>\n",
        "**6) ReAct Prompting(Reason and Act)** <br>"
      ],
      "metadata": {
        "id": "26TpFzFpFPSG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyLz5MA_EtXJ"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpgiMLebGPJ7",
        "outputId": "bdd543c2-1805-4490-8696-7115c8eb8113"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-turn Conversation (Chatbot with conversation history)"
      ],
      "metadata": {
        "id": "eVgsswo6TYAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "# Initialize the conversation with a system greeting\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "]"
      ],
      "metadata": {
        "id": "RdK-bQA7G784"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate AI response based on the conversation\n",
        "def generate_response(conversation):\n",
        "    try:\n",
        "        # Use the OpenAI ChatCompletion API to generate a response\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",  # Use the GPT-3.5 Turbo engine or the appropriate model\n",
        "            messages=conversation,\n",
        "            temperature=0.7,  # Controls the randomness of the response\n",
        "            top_p=0.5,      # Controls the diversity of the response\n",
        "            n=1,           # Number of responses to generate\n",
        "            stream=False,  # If True, the API returns a stream of responses; if False, it returns a single response\n",
        "            presence_penalty=0.3,  # This parameter nudges the model to include a wide variety of tokens in the generated text\n",
        "            frequency_penalty=0.5,  # Controls how much the model avoids repeating the same response\n",
        "        )\n",
        "\n",
        "        # Extract the content of the AI's reply\n",
        "        ai_reply = response.choices[0].message.content.strip()\n",
        "\n",
        "        return ai_reply\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions that might occur during API request\n",
        "        return \"Sorry, I encountered an error.\"\n"
      ],
      "metadata": {
        "id": "5N90-CXyHLbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop for user interaction\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check if the user wants to exit the conversation\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Add user's input to the conversation\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Generate AI's response based on the updated conversation\n",
        "    assistant_reply = generate_response(conversation)\n",
        "\n",
        "    # Print the AI's rewhysponse\n",
        "    print(\"Assistant:\", assistant_reply)\n",
        "    # What are fun activities I can do this weekend?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZojOl90H1Nz",
        "outputId": "57d1e049-7a1e-4716-a982-7e80ad5ec688"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: What are fun activities I can do this weekend?\n",
            "Assistant: There are plenty of fun activities you can do this weekend! Here are some ideas:\n",
            "\n",
            "1. Visit a local museum or art gallery.\n",
            "2. Go for a hike or nature walk in a nearby park.\n",
            "3. Have a picnic at a scenic spot.\n",
            "4. Try out a new restaurant or café in your area.\n",
            "5. Attend a live music performance or theater show.\n",
            "6. Have a movie night at home with friends or family.\n",
            "7. Take a day trip to a nearby town or city you've never been to before.\n",
            "8. Go shopping at a local market or boutique.\n",
            "9. Have a DIY craft day and make something creative.\n",
            "10. Volunteer for a cause you care about.\n",
            "\n",
            "I hope these suggestions help you have an enjoyable weekend!\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero-shot prompting**\n",
        "\n",
        "Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it."
      ],
      "metadata": {
        "id": "IV76I9qwIQeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''\n",
        "Message: Hi alex, thanks for the thoughtful birthday card!\n",
        "Sentiment: ?\n",
        "'''\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGtltyWkH5Hu",
        "outputId": "2ff2b1e9-6663-434f-f7ae-0510f4ffe25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the message is positive. It expresses gratitude and appreciation for the thoughtful birthday card received from Alex.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Few-shot prompting**\n",
        "\n",
        "Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response."
      ],
      "metadata": {
        "id": "uJz9R8erKb3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
        "Sentiment: Negative\n",
        "\n",
        "Message: Can't wait to order pizza for dinner tonight\n",
        "Sentiment: Positive\n",
        "\n",
        "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
        "Sentiment: ?\n",
        "\"\"\"\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfn3Zh3HJnBQ",
        "outputId": "0d3e9c15-bdbb-4202-93ea-710439516157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specifying the Output Format\n",
        "- You can also specify the format in which you want the model to respond.\n",
        "- In the example below, you are asking to \"give a one word response\"."
      ],
      "metadata": {
        "id": "J3F2X283K2nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
        "Sentiment: Negative\n",
        "\n",
        "Message: Can't wait to order pizza for dinner tonight\n",
        "Sentiment: Positive\n",
        "\n",
        "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
        "Sentiment: ?\n",
        "\n",
        "Give a one word response.\n",
        "\"\"\"\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E26iKMrKYK0",
        "outputId": "13522bab-f0fb-49ef-e6a7-b6af4a0317b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chain-of-Thought Prompting**\n",
        "chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
      ],
      "metadata": {
        "id": "6z21K3uzVN8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\n",
        "\"\"\"\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPE0hYEUMGkX",
        "outputId": "bf59ebc4-e6e0-4e1e-d164-63dfc0f3b719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tree of Thoughts (ToT)**\n",
        "\n",
        "Tree of Thoughts (ToT) prompting is a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that serve as intermediate steps for general problem-solving with language models.\n",
        "\n",
        "\n",
        "How does it work?\n",
        "\n",
        "- ToT prompting breaks problems down into smaller parts, similar to CoT prompting, but goes further by combining this with the ability to explore multiple solution paths in parallel, forming a tree instead of a single chain. Each thought is generated or solved independently and passed to the next step, allowing the model to self-evaluate and decide whether to continue with that path or choose another."
      ],
      "metadata": {
        "id": "bQvoNRxqVo5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"\"\"\n",
        "Imagine 5 different experts are answering this question.\n",
        "They will brainstorm the answer step by step reasoning carefully and taking all facts into consideration\n",
        "All experts will write down 1 step of their thinking,\n",
        "then share it with the group.\n",
        "They will each critique their response, and the all the responses of others\n",
        "They will check their answer based on science and the laws of physics\n",
        "They will keep going through steps until they reach their conclusion taking into account the thoughts of the other experts\n",
        "If at any time they realise that there is a flaw in their logic they will backtrack to where that flaw occurred\n",
        "If any expert realises they're wrong at any point then they acknowledges this and start another train of thought\n",
        "Each expert will assign a likelihood of their current assertion being correct\n",
        "Continue until the experts agree on the single most likely location\n",
        "The question is.\n",
        "1. Carlos is at the swimming pool.\n",
        "2. He walks to the locker room, carrying a towel.\n",
        "3. He puts his watch in the towel and carries the towel tightly to a lounger at the poolside.\n",
        "4. At the lounger he opens and vigorously shakes the towel, then walks to the snack bar.\n",
        "5. He leaves the towel at the snack bar, then walks to the diving board.\n",
        "6. Later Carlos realises he has has lost his watch. Where is the single most likely location of the watch?\n",
        "\"\"\"\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF7_WBM9MT78",
        "outputId": "5ce2617b-1e85-429c-b082-6185c9cc39fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expert 1: The most likely location of the watch is at the snack bar where Carlos left the towel.\n",
            "\n",
            "Critique: This assumption is based on the fact that Carlos was last seen with the towel at the snack bar. However, we should also consider other possibilities such as whether he could have dropped the watch during any of his movements.\n",
            "\n",
            "Likelihood: 60%\n",
            "\n",
            "Expert 2: Let's backtrack a bit and consider if there were any opportunities for Carlos to lose his watch before reaching the snack bar. It's possible that he could have dropped it while vigorously shaking the towel at the lounger.\n",
            "\n",
            "Critique: This is a valid point to consider. We need to carefully analyze each step of Carlos' actions to determine where he might have lost his watch.\n",
            "\n",
            "Likelihood: 40%\n",
            "\n",
            "Expert 3: Building on Expert 2's point, it's also worth considering if there were any potential moments during his walk from the locker room to the lounger where he could have misplaced his watch. Perhaps it fell out of the towel during this time.\n",
            "\n",
            "Critique: Agreed, we should not overlook any part of Carlos' journey as there could have been multiple opportunities for him to lose his watch.\n",
            "\n",
            "Likelihood: 30%\n",
            "\n",
            "Expert 4: Continuing from Expert 3's point, we should also take into account the possibility that Carlos may have lost his watch while walking from the lounger to the snack bar. It's important to thoroughly examine all transitions between locations.\n",
            "\n",
            "Critique: That's a valid consideration. We must ensure we cover all bases in our analysis before reaching a conclusion on the most likely location of the watch.\n",
            "\n",
            "Likelihood: 25%\n",
            "\n",
            "Expert 5: Considering all previous points made by experts, I believe that based on Carlos' actions and movements, it is most likely that he lost his watch either while vigorously shaking the towel at the lounger or during his walk from there to the snack bar. These are critical moments where he could have accidentally dropped or misplaced it.\n",
            "\n",
            "Critique: This seems like a reasonable conclusion based on our thorough analysis of each step in Carlos' journey. We have considered various scenarios and potential points of loss for the watch.\n",
            "\n",
            "Likelihood: 70%\n",
            "\n",
            "Conclusion:\n",
            "After careful consideration and analysis by all experts, we collectively agree that the single most likely location of Carlos' lost watch is either at the lounger where he vigorously shook his towel or during his walk from there to the snack bar. These moments present high-risk scenarios where he could have unknowingly dropped or misplaced it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReAct Prompting**\n",
        "\n",
        "ReAct is a framework where LLMs are used to generate both reasoning traces and task-specific actions in an interleaved manner.\n",
        "\n",
        "Generating reasoning traces allow the model to induce, track, and update action plans, and even handle exceptions. The action step allows to interface with and gather information from external sources such as knowledge bases or environments."
      ],
      "metadata": {
        "id": "i3Sfro9fQ1sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "I want to learn generative AI, give a good roadmap for it\n",
        "\"\"\"\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant, Solve a question answering task with interleaving Thought, Action, observation steps\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u1q_MyZO68A",
        "outputId": "0fbdca80-b663-4139-94a5-852347e8331d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: Generative AI is a fascinating field that involves creating models capable of generating new data, such as images, text, or music. To learn generative AI effectively, it's important to follow a structured roadmap that covers both theoretical concepts and practical implementation.\n",
            "\n",
            "Action:\n",
            "1. Start by building a strong foundation in machine learning and deep learning. Learn about neural networks, optimization algorithms, and common deep learning frameworks like TensorFlow or PyTorch.\n",
            "2. Dive into the specific subfields of generative AI, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and autoregressive models.\n",
            "3. Study key papers and research in generative AI to understand the latest advancements in the field. Implement some of these models from scratch to deepen your understanding.\n",
            "4. Experiment with different datasets and model architectures to gain hands-on experience with generative AI techniques.\n",
            "5. Join online communities and forums dedicated to generative AI, such as Reddit's r/MachineLearning or OpenAI's blog, to stay updated on the latest trends and research.\n",
            "\n",
            "Observation: By following this roadmap, you will gradually build expertise in generative AI and be able to create your own innovative projects in this exciting field. Remember that practice and persistence are key to mastering generative AI techniques. Good luck on your learning journey!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization\n",
        "- Summarizing a large text is another common use case for LLMs. Let's try that!"
      ],
      "metadata": {
        "id": "yWNC1aQETkQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "email = \"\"\"\n",
        "Dear Alex,\n",
        "\n",
        "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n",
        "\n",
        "Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n",
        "\n",
        "Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\n",
        "One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task — the input and the desired output — sometimes yields better results.\n",
        "Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\n",
        "Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\n",
        "For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you’re unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn’t work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn’t deliver the performance you want, then try fine-tuning — but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n",
        "\n",
        "(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? 😜)\n",
        "\n",
        "Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that’s not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM’s output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning — in which GPT-4 surpasses current open models — it can be difficult to fine-tune a smaller model to deliver superior results.\n",
        "\n",
        "Beyond choosing a development approach, it’s also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I’ll talk about how to make this choice in a future letter.\n",
        "\n",
        "Keep learning!\n",
        "\n",
        "Andrew\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bwPb4FRsTIz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Summarize this email and extract some key points.\n",
        "What did the author say about llama models?:\n",
        "\n",
        "email: {email}\n",
        "\"\"\"\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31VcLqCYTvBI",
        "outputId": "c8c69792-084a-4917-b97a-a3ccff682073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "The email discusses different ways to build applications based on large language models (LLMs), such as prompting, one-shot or few-shot prompting, fine-tuning, and pretraining. The author recommends starting with prompting for quick application development and gradually moving to more complex techniques if needed. The email also mentions the importance of choosing the right model based on processing power, knowledge about the world, and reasoning ability.\n",
            "\n",
            "Key Points about Llama Models:\n",
            "- Llama models are a type of large language model (LLM).\n",
            "- The author mentions that a member of the team has been trying to fine-tune Llama-2-7B to sound like them.\n",
            "- The email does not provide specific details about Llama models but focuses more on general approaches to building applications using LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Give the model time to “think”**\n",
        "\n",
        "Tactic 1: Specify the steps required to complete a task"
      ],
      "metadata": {
        "id": "jzD8TCwDZnlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "In a charming village, siblings Jack and Jill set out on \\\n",
        "a quest to fetch water from a hilltop \\\n",
        "well. As they climbed, singing joyfully, misfortune \\\n",
        "struck—Jack tripped on a stone and tumbled \\\n",
        "down the hill, with Jill following suit. \\\n",
        "Though slightly battered, the pair returned home to \\\n",
        "comforting embraces. Despite the mishap, \\\n",
        "their adventurous spirits remained undimmed, and they \\\n",
        "continued exploring with delight.\n",
        "\"\"\"\n",
        "# example 1\n",
        "prompt = f\"\"\"\n",
        "Perform the following actions:\n",
        "1 - Summarize the following text delimited by triple \\\n",
        "backticks with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the following \\\n",
        "keys: french_summary, num_names.\n",
        "\n",
        "Separate your answers with line breaks.\n",
        "\n",
        "Text:\n",
        "```{text}```\n",
        "\"\"\"\n",
        "\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J96xp9Z5T1VS",
        "outputId": "ce08ca6c-9dfe-465a-ad85-09e9ab2f4bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 - Summary: Jack and Jill, siblings on a quest to fetch water, face misfortune but return home with adventurous spirits intact.\n",
            "\n",
            "2 - French Translation: Jack et Jill, frère et sœur en quête d'eau, affrontent la malchance mais rentrent chez eux avec des esprits aventureux intacts.\n",
            "\n",
            "3 - Names in French Summary: Jack, Jill\n",
            "\n",
            "4 - JSON Object:\n",
            "{\n",
            "  \"french_summary\": \"Jack et Jill, frère et sœur en quête d'eau, affrontent la malchance mais rentrent chez eux avec des esprits aventureux intacts.\",\n",
            "  \"num_names\": 2\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion"
      ],
      "metadata": {
        "id": "uvrSyc5GZwx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Determine if the student's solution is correct or not.\n",
        "\n",
        "Question:\n",
        "I'm building a solar power installation and I need \\\n",
        " help working out the financials.\n",
        "- Land costs $100 / square foot\n",
        "- I can buy solar panels for $250 / square foot\n",
        "- I negotiated a contract for maintenance that will cost \\\n",
        "me a flat $100k per year, and an additional $10 / square \\\n",
        "foot\n",
        "What is the total cost for the first year of operations\n",
        "as a function of the number of square feet.\n",
        "\n",
        "Student's Solution:\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "1. Land cost: 100x\n",
        "2. Solar panel cost: 250x\n",
        "3. Maintenance cost: 100,000 + 100x\n",
        "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
        "\"\"\"\n",
        "\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9FskGh2Y_xy",
        "outputId": "a684d18c-5682-476f-d877-f53dd943c807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The student's solution is incorrect. The total cost for the first year of operations should be calculated as follows:\n",
            "\n",
            "1. Land cost: $100 per square foot * x square feet = $100x\n",
            "2. Solar panel cost: $250 per square foot * x square feet = $250x\n",
            "3. Maintenance cost: $100,000 (flat fee) + $10 per square foot * x square feet = $100,000 + $10x\n",
            "\n",
            "Total cost for the first year of operations = Land cost + Solar panel cost + Maintenance cost\n",
            "Total cost = $100x + $250x + ($100,000 + $10x)\n",
            "Total cost = $350x + $100,000\n",
            "\n",
            "Therefore, the correct total cost for the first year of operations as a function of the number of square feet is 350x + 100,000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Managing Halluciation**"
      ],
      "metadata": {
        "id": "T6IMqgVIfdLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Managing LLM Hallucination by giving proper context**\n",
        "\n",
        "One of the reason for hallucinations in large language models is the lack of knowledge in certain areas. By integrating an external knowledge base, we can reduce the occurrence of these hallucinations."
      ],
      "metadata": {
        "id": "fhwmJ66Ykhe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f'''\n",
        "\n",
        "Q. who is the nobel prize winner in physics in 2023?\n",
        "\n",
        "'''\n",
        "\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6Fwg6ABbfFH",
        "outputId": "a5bfb617-3bd8-48ca-8970-55469299d695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but I am unable to provide real-time information as my data is not up to date. I recommend checking the official Nobel Prize website or news sources for the most recent information on the Nobel Prize winners in Physics for 2023.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_corpus = '''Ferenc Krausz (born 17 May 1962[2]) is an Austrian-Hungarian physicist working in attosecond science.\n",
        "He is a director at the Max Planck Institute of Quantum Optics and a professor of experimental physics at the Ludwig Maximilian University of Munich in Germany.\n",
        "His research team has generated and measured the first attosecond light pulse and used it for capturing electrons' motion\n",
        "inside atoms, marking the birth of attophysics.[2] In 2023, jointly with Pierre Agostini and Anne L'Huillier, he was awarded the Nobel Prize in Physics.'''\n",
        "\n",
        "prompt=f'''\n",
        "\n",
        "context={text_corpus}\n",
        "\n",
        "Q. who is the nobel prize winner in physics?\n",
        "\n",
        "'''\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqhH7cBBZO4q",
        "outputId": "c236f44b-7ae2-45ca-8dce-03ac7f6eb98f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Nobel Prize in Physics in 2023 was awarded jointly to Ferenc Krausz, Pierre Agostini, and Anne L'Huillier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Chain-of-Verification Prompting**\n",
        "\n",
        "The Chain-of-Verification (CoVe) prompt engineering method aims to reduce hallucinations through a verification loop."
      ],
      "metadata": {
        "id": "dPnsQc3hpQ7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question=\"Name some athletes who were born in United states\"\n",
        "\n",
        "prompt=f'''Here is the question: {Question}.\n",
        "\n",
        "First, generate a response.\n",
        "\n",
        "Then, create and answer verification questions based on this response to check for accuracy. Think it through and make sure you are extremely accurate based on the question asked.\n",
        "\n",
        "After answering each verification question, consider these answers and revise the initial response to formulate a final, verified answer. Ensure the final response reflects the accuracy and findings from the verification process.'''\n",
        "\n",
        "\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9mjZtAJfDdm",
        "outputId": "a8baa5b5-796a-41b2-e727-d0baa6cdd485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Some athletes who were born in the United States include Michael Jordan, Serena Williams, LeBron James, Tom Brady, and Simone Biles.\n",
            "\n",
            "Verification Question 1: Is Michael Jordan an athlete born in the United States?\n",
            "Answer: Yes.\n",
            "\n",
            "Verification Question 2: Is Cristiano Ronaldo an athlete born in the United States?\n",
            "Answer: No.\n",
            "\n",
            "Verification Question 3: Are Serena Williams and Venus Williams both athletes born in the United States?\n",
            "Answer: Yes.\n",
            "\n",
            "Final Verified Answer: Some athletes who were born in the United States are Michael Jordan, Serena Williams, LeBron James, Tom Brady, and Simone Biles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Step-Back Prompting**\n",
        "\n",
        "A crucial rule to remember when prompting LLMs is to give them space to 'think'. You don’t want to overly constrain the model such that it can’t explore various solutions.\n",
        "\n",
        "Chain of thought reasoning is one way to push the model to think through the problem. A simple way to implement this type of reasoning is to add a statement like “think through this task step-by-step” at the end of your prompt."
      ],
      "metadata": {
        "id": "YEF7Xk88tIQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question=\"What is Quantum Physics?\"\n",
        "prompt=f'''Here is a question or task: {Question}\n",
        "\n",
        "Let's think step-by-step to answer this:\n",
        "\n",
        "Step 1) Abstract the key concepts and principles relevant to this question:\n",
        "\n",
        "Step 2) Use the abstractions to reason through the question:\n",
        "\n",
        "Final Answer: '''\n",
        "\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "ans=generate_response(messages)\n",
        "\n",
        "print(ans)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqmZDv7ApGI6",
        "outputId": "2198d36e-0ecf-44e7-d32a-6341c731cce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1) Key concepts and principles of Quantum Physics:\n",
            "- Wave-particle duality\n",
            "- Superposition\n",
            "- Entanglement\n",
            "- Uncertainty principle\n",
            "- Quantum tunneling\n",
            "\n",
            "Step 2) Reasoning through the question:\n",
            "Quantum Physics is a branch of physics that deals with the behavior of matter and energy at the smallest scales of atoms and subatomic particles. It describes phenomena such as wave-particle duality, where particles exhibit both wave-like and particle-like properties, superposition, where particles can exist in multiple states simultaneously until measured, entanglement, where particles become correlated in a way that their quantum states are dependent on each other, uncertainty principle, which states that certain pairs of physical properties cannot be precisely determined simultaneously, and quantum tunneling, where particles can pass through energy barriers that would be impossible according to classical physics.\n",
            "\n",
            "Final Answer: Quantum Physics is the branch of physics that studies the behavior of matter and energy at the smallest scales using principles such as wave-particle duality, superposition, entanglement, uncertainty principle, and quantum tunneling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UfoRBs-os3rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thank You"
      ],
      "metadata": {
        "id": "23-8_wu6QI-j"
      }
    }
  ]
}