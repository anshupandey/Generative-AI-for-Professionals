{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-for-Professionals/blob/main/langchain-course/01_Function_Calling_with_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qm8x2zhhd-8"
      },
      "source": [
        "# Integrating tools/function with LLM using OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why function calling?\n",
        "\n",
        "When working with generative text models, it can be difficult to coerce generative models to give consistent outputs in a structured format such as JSON. Function Calling in Gemini allows you to overcome this limitation by forcing the model to output structured data in the format and schema that you define.\n",
        "\n",
        "You can think of Function Calling as a way to get structured output from user prompts and function definitions, use that structured output to make an API request to an external system, then return the function response to the generative model so that it can generate a natural language summary. In other words, function calling in Gemini helps you go from unstructured text in prompt, to a structured data object, and back to natural language again."
      ],
      "metadata": {
        "id": "JNQShhNqBQmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benefits of Function Calling\n",
        "\n",
        "- **Native framework**: Function Calling is a native framework in Gemini, so there's no need to enable additional APIs or install extra packages.\n",
        "- **Time savings**: No need to write custom code to call, parse, and synthesize information from multiple APIs.\n",
        "- **Simplified interaction with generative AI models**: Gemini handles the complex task of understanding the user's intent, predicting function calls, extracting relevant function parameters, and generating natural language summaries.\n",
        "- **Versatility**: Easily extend capabilities by adding more function calls for different APIs and tailoring it to your needs.\n"
      ],
      "metadata": {
        "id": "1HONIxlaBkvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu6uxcLphsRz",
        "outputId": "94cef08d-8d7c-47f8-fb70-a9044cf643f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTT0Jc-qhVby"
      },
      "outputs": [],
      "source": [
        "# Define constants for the Azure OpenAI API configuration\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"YOUR_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLKRALkuhVby"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "import requests\n",
        "\n",
        "client = OpenAI()\n",
        "model_name = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMs1SliPer2Q"
      },
      "source": [
        "In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call one or many functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6voT6dEZOEQ"
      },
      "source": [
        "## Supported models\n",
        "Not all model versions are trained with function calling data. Function calling is supported with the following models: gpt-4, gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4-0613, gpt-3.5-turbo, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, and gpt-3.5-turbo-0613\n",
        "\n",
        "In addition, parallel function calls is supported on the following models: gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-3.5-turbo-0125, and gpt-3.5-turbo-1106"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUAVQTH1FCob"
      },
      "outputs": [],
      "source": [
        "# Define the OpenWeatherMap API key\n",
        "OWM_API_KEY = \"29af1cea50a401d8e624eea4660b3f59\"\n",
        "\n",
        "def get_current_weather(location, unit=\"kelvin\"):\n",
        "    \"\"\"\n",
        "    Fetches the current weather information for a given location.\n",
        "\n",
        "    Parameters:\n",
        "    - location: str, the name of the location (e.g., \"Paris\").\n",
        "    - unit: str, the unit of temperature (default is \"kelvin\").\n",
        "\n",
        "    Returns:\n",
        "    - str: JSON formatted string containing weather information.\n",
        "    \"\"\"\n",
        "    # Construct the API request URL\n",
        "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={OWM_API_KEY}\"\n",
        "\n",
        "    # Send the API request\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the temperature and weather forecast from the response\n",
        "    temp = response.json()['main']['temp']\n",
        "    forecast = [response.json()['weather'][0]['main'], response.json()['weather'][0]['description']]\n",
        "\n",
        "    # Create a dictionary with the weather information\n",
        "    weather_info = {\n",
        "        \"location\": location,\n",
        "        \"temperature\": temp,\n",
        "        \"unit\": 'Kelvin',\n",
        "        \"forecast\": forecast\n",
        "    }\n",
        "\n",
        "    # Return the weather information as a JSON string\n",
        "    return json.dumps(weather_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zvpewsATGB30",
        "outputId": "6a05fd0f-f4d4-43f9-f0e3-f3b29d98dc5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"location\": \"Paris\", \"temperature\": 291.17, \"unit\": \"Kelvin\", \"forecast\": [\"Clear\", \"clear sky\"]}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "get_current_weather(\"Paris\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sTPInk2CiVpV",
        "outputId": "b62bbfa7-c88f-4bca-ba43-75e4cd4fff9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"location\": \"Manila\", \"temperature\": 302.12, \"unit\": \"Kelvin\", \"forecast\": [\"Rain\", \"light rain\"]}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "get_current_weather(\"Manila\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OLj1VFabUCR",
        "outputId": "2273a170-ef0c-4751-b652-fb1ec82da889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-9jdO58mgJUIxSu9Pjsx89ohncVUxi\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, typically computer systems. This includes processes such as learning, reasoning, problem-solving, perception, and language understanding. AI technologies can be used in a wide range of applications, from virtual assistants like Siri and Alexa to self-driving cars and predictive analytics.\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": null\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1720663193,\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"service_tier\": null,\n",
            "  \"system_fingerprint\": null,\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 66,\n",
            "    \"prompt_tokens\": 11,\n",
            "    \"total_tokens\": 77\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\":\"user\",'content':\"what is AI?\"}]\n",
        "results = client.chat.completions.create(model = model_name, messages = messages)\n",
        "print(results.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l316C1xAb-Qy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define tools/functions for the LLM to use\n",
        "tools = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"get_current_weather\",\n",
        "                \"description\": \"Get the current weather in a given location\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"location\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                        },\n",
        "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\",\"kelvin\"]},\n",
        "                    },\n",
        "                    \"required\": [\"location\"],\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gdvGUJGbheA"
      },
      "outputs": [],
      "source": [
        "def get_response(messages, tools, model=model_name):\n",
        "    \"\"\"\n",
        "    Handles interaction with the LLM, including making function calls if needed.\n",
        "\n",
        "    Parameters:\n",
        "    - messages: list, the conversation history.\n",
        "    - tools: list, the functions/tools available to the LLM.\n",
        "    - model: str, the model name to use (default is model_name).\n",
        "\n",
        "    Returns:\n",
        "    - str: The response from the LLM or function.\n",
        "    \"\"\"\n",
        "    # Create a chat completion with the given messages and tools\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice='auto',\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    #print(response.model_dump_json(indent=2))\n",
        "    # Get the generated response and tool calls (if any)\n",
        "    response = response.choices[0].message\n",
        "    tool_calls = response.tool_calls\n",
        "\n",
        "    try:\n",
        "        if tool_calls:\n",
        "            print(\"Making a function call\")\n",
        "            # Available functions\n",
        "            available_functions = {\n",
        "                \"get_current_weather\": get_current_weather,\n",
        "            }\n",
        "            print(tool_calls)\n",
        "\n",
        "            # Extend conversation with assistant's reply\n",
        "            messages.append(response)\n",
        "\n",
        "            # Handle each function call\n",
        "            for tool_call in tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_to_call = available_functions[function_name]\n",
        "                function_args = json.loads(tool_call.function.arguments)\n",
        "                function_response = function_to_call(**function_args)\n",
        "                messages.append(\n",
        "                    {\n",
        "                        \"tool_call_id\": tool_call.id,\n",
        "                        \"role\": \"tool\",\n",
        "                        \"name\": function_name,\n",
        "                        \"content\": function_response,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            # Get a new response from the model with the function response\n",
        "            second_response = client.chat.completions.create(\n",
        "                model=model_name,\n",
        "                messages=messages,\n",
        "            )\n",
        "            return second_response\n",
        "        else:\n",
        "            return response.content\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred\", e)\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWL0wVqFdeBm",
        "outputId": "fcbd0ba8-8baf-4b71-8c01-08970406d9a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI, or Artificial Intelligence, is the simulation of human intelligence processes by machines, especially computer systems. It involves the development of algorithms that enable machines to perform tasks that typically require human intelligence.\n"
          ]
        }
      ],
      "source": [
        "# Example usage of get_response function\n",
        "messages = [{\"role\": \"user\", \"content\": \"Provide a 2 line explanation for AI\"}]\n",
        "response = get_response(messages, tools)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0H4DaivdmPW",
        "outputId": "45d28159-4931-4f7c-b2ed-cf5131ef2261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making a function call\n",
            "[ChatCompletionMessageToolCall(id='call_5YlkdEcRaGJVviUkcnz9VadL', function=Function(arguments='{\"location\":\"Tokyo\",\"unit\":\"celsius\"}', name='get_current_weather'), type='function')]\n",
            "The weather in Tokyo today is mostly cloudy with broken clouds, and the temperature is around 30.91°C.\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"How is the weather in Tokyo today?\"}]\n",
        "response = get_response(messages, tools)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iaZlvyyQk8SW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}