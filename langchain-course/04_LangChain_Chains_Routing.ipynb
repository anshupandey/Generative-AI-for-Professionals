{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-for-Professionals/blob/main/langchain-course/04_LangChain_Chains_Routing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-core langchain-community langgraph langchain-openai"
      ],
      "metadata": {
        "id": "r7W4FdeobQ_R",
        "outputId": "7d2b047f-c6e2-42fe-adcc-bd9c176e8741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "r7W4FdeobQ_R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/366.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m286.7/366.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.4/91.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "injcJIQwhpuz",
        "outputId": "2f94d1c5-f9ef-442d-eb1b-ada77932bb43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "injcJIQwhpuz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b47436a",
      "metadata": {
        "id": "4b47436a"
      },
      "source": [
        "# How to route between sub-chains\n",
        "\n",
        ":::info Prerequisites\n",
        "\n",
        "This guide assumes familiarity with the following concepts:\n",
        "- [LangChain Expression Language (LCEL)](/docs/concepts/#langchain-expression-language)\n",
        "- [Chaining runnables](/docs/how_to/sequence/)\n",
        "- [Configuring chain parameters at runtime](/docs/how_to/configure)\n",
        "- [Prompt templates](/docs/concepts/#prompt-templates)\n",
        "- [Chat Messages](/docs/concepts/#message-types)\n",
        "\n",
        ":::\n",
        "\n",
        "Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing can help provide structure and consistency around interactions with models by allowing you to define states and use information related to those states as context to model calls.\n",
        "\n",
        "There are two ways to perform routing:\n",
        "\n",
        "1. Conditionally return runnables from a [`RunnableLambda`](/docs/how_to/functions) (recommended)\n",
        "2. Using a `RunnableBranch` (legacy)\n",
        "\n",
        "We'll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\""
      ],
      "metadata": {
        "id": "R3XXribGbMx4"
      },
      "id": "R3XXribGbMx4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c1c6edac",
      "metadata": {
        "id": "c1c6edac"
      },
      "source": [
        "## Example Setup\n",
        "First, let's create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a8a1967",
      "metadata": {
        "id": "8a8a1967",
        "outputId": "13bbe756-f42c-45dc-b36c-ca89e7c61d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gemini \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0.5)\n",
        "\n",
        "\n",
        "chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Gemini`, or `Other`.\n",
        "\n",
        "Do not respond with more than one word.\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Classification:\"\"\"\n",
        "    )\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"how do I call Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ],
      "metadata": {
        "id": "rhpM0hSvPe4J",
        "outputId": "e9a5d8d3-fa84-449d-d550-0466c3462df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "rhpM0hSvPe4J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"what are the models available with OpenAI API?\"})"
      ],
      "metadata": {
        "id": "KjaNGHrGPivP",
        "outputId": "7c01fff5-4ba5-4019-c91c-97656cd2771d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "id": "KjaNGHrGPivP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Other \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7655555f",
      "metadata": {
        "id": "7655555f"
      },
      "source": [
        "Now, let's create three sub chains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d7722d",
      "metadata": {
        "id": "89d7722d"
      },
      "outputs": [],
      "source": [
        "langchain_chain = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert in langchain. \\\n",
        "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
        "Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model\n",
        "\n",
        "\n",
        "\n",
        "gemini_chain = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert in Vertex AI Gemini Models. \\\n",
        "Always answer questions starting with \"As Sundar Pichai told me\". \\\n",
        "Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model\n",
        "\n",
        "\n",
        "general_chain = PromptTemplate.from_template(\n",
        "    \"\"\"Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8d042c",
      "metadata": {
        "id": "6d8d042c"
      },
      "source": [
        "## Using a custom function (Recommended)\n",
        "\n",
        "You can also use a custom function to route between different outputs. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687492da",
      "metadata": {
        "id": "687492da"
      },
      "outputs": [],
      "source": [
        "def route(info):\n",
        "    if \"gemini\" in info[\"topic\"].lower():\n",
        "        return gemini_chain\n",
        "    elif \"langchain\" in info[\"topic\"].lower():\n",
        "        return langchain_chain\n",
        "    else:\n",
        "        return general_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02a33c86",
      "metadata": {
        "id": "02a33c86"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
        "    route\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2e977a4",
      "metadata": {
        "id": "c2e977a4",
        "outputId": "83c00a3b-e0a9-4297-eff2-4d7182117a63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Sundar Pichai told me, Gemini is still under development and not yet publicly available. However, you can stay updated on its progress and potential future uses by following Google AI's announcements and publications. \\n\", response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}], 'usage_metadata': {'prompt_token_count': 42, 'candidates_token_count': 44, 'total_token_count': 86}}, id='run-67e1d321-9d02-4688-ac5e-9cb7c5ea42da-0', usage_metadata={'input_tokens': 42, 'output_tokens': 44, 'total_tokens': 86})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48913dc6",
      "metadata": {
        "id": "48913dc6",
        "outputId": "b6d54820-821b-4d28-e833-a7a2c4666d19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Harrison Chase told me, LangChain is a powerful framework for building applications that use large language models (LLMs). Here's a breakdown of how to use it:\\n\\n**1. Installation:**\\n\\n```bash\\npip install langchain\\n```\\n\\n**2. Core Concepts:**\\n\\n* **Chains:** These are the building blocks of LangChain applications. They combine different components (LLMs, prompts, tools) to perform specific tasks.\\n* **LLMs:** LangChain supports various LLMs, including OpenAI's GPT-3, Google's PaLM, and others.\\n* **Prompts:** You\", response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}], 'usage_metadata': {'prompt_token_count': 39, 'candidates_token_count': 128, 'total_token_count': 167}}, id='run-937bccda-292e-401c-a9a6-df6938e7254b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 128, 'total_tokens': 167})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a14d0dca",
      "metadata": {
        "id": "a14d0dca",
        "outputId": "c5f2fff8-00c9-41b1-8293-39e5d3cc0719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='2 + 2 = 4 \\n', response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}], 'usage_metadata': {'prompt_token_count': 18, 'candidates_token_count': 9, 'total_token_count': 27}}, id='run-08348818-4a03-44db-ab13-1b0d746f674f-0', usage_metadata={'input_tokens': 18, 'output_tokens': 9, 'total_tokens': 27})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5147b827",
      "metadata": {
        "id": "5147b827"
      },
      "source": [
        "## Using a RunnableBranch\n",
        "\n",
        "A `RunnableBranch` is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does **not** offer anything that you can't achieve in a custom function as described above, so we recommend using a custom function instead.\n",
        "\n",
        "A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n",
        "\n",
        "If no provided conditions match, it runs the default runnable.\n",
        "\n",
        "Here's an example of what it looks like in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a101418",
      "metadata": {
        "id": "2a101418",
        "outputId": "eb836e77-3875-4b70-fcf8-61db67f31701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Sundar Pichai told me, Gemini is still under development and not yet publicly available. However, you can stay updated on its progress and potential future uses by following Google AI's announcements and publications. \\n\", response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}], 'usage_metadata': {'prompt_token_count': 42, 'candidates_token_count': 44, 'total_token_count': 86}}, id='run-cc6f924f-66a7-4fa5-856d-418110cbf927-0', usage_metadata={'input_tokens': 42, 'output_tokens': 44, 'total_tokens': 86})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableBranch\n",
        "\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"gemini\" in x[\"topic\"].lower(), gemini_chain),\n",
        "    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
        "    general_chain,\n",
        ")\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch\n",
        "full_chain.invoke({\"question\": \"how do I use Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8caf9b",
      "metadata": {
        "id": "8d8caf9b",
        "outputId": "03f64370-3c05-4adb-c463-27101833fa2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Harrison Chase told me, LangChain is a powerful framework for building applications that use large language models (LLMs). Here's a breakdown of how to use it:\\n\\n**1. Installation:**\\n\\n```bash\\npip install langchain\\n```\\n\\n**2. Core Concepts:**\\n\\n* **Chains:** These are sequences of components that work together to achieve a specific task. For example, a chain might involve retrieving information from a database, summarizing it, and then generating a response.\\n* **LLMs:** LangChain integrates with various LLMs, including OpenAI's GPT-3, Google's PaLM,\", response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}], 'usage_metadata': {'prompt_token_count': 39, 'candidates_token_count': 128, 'total_token_count': 167}}, id='run-7e7444a2-8686-4ef3-b50f-226e209d9603-0', usage_metadata={'input_tokens': 39, 'output_tokens': 128, 'total_tokens': 167})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26159af7",
      "metadata": {
        "id": "26159af7",
        "outputId": "47b7e252-3a52-4bd7-e856-f912203e9d00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='2 + 2 = 4 \\n', response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}], 'usage_metadata': {'prompt_token_count': 18, 'candidates_token_count': 9, 'total_token_count': 27}}, id='run-a814d885-cee7-422c-a8ff-e6f5809ad573-0', usage_metadata={'input_tokens': 18, 'output_tokens': 9, 'total_tokens': 27})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff40bcb3",
      "metadata": {
        "id": "ff40bcb3"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "You've now learned how to add routing to your composed LCEL chains.\n",
        "\n",
        "Next, check out the other how-to guides on runnables in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927b7498",
      "metadata": {
        "id": "927b7498"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}