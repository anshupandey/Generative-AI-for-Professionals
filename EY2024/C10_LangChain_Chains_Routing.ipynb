{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-for-Professionals/blob/main/EY2024/C10_LangChain_Chains_Routing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-core langchain-community langgraph langchain-openai"
      ],
      "metadata": {
        "id": "r7W4FdeobQ_R",
        "outputId": "ae6e2695-f5f2-4c1d-b3bd-970f0cfc6bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "r7W4FdeobQ_R",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "injcJIQwhpuz",
        "outputId": "5da1224b-a44c-4a3b-9f69-ae3de45cd0da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "injcJIQwhpuz",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"xxxxxxxxxxxxxxxxxxxxx\"\n",
        "api_version = \"2024-02-01\"\n",
        "azure_endpoint = \"https://xxxxxxxxxxx.openai.azure.com/\"\n",
        "model_name = \"gpt-4o\"\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_VERSION\"] = api_version\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_endpoint\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = api_key\n",
        "os.environ[\"OPENWEATHERMAP_API_KEY\"] = \"29af1cea50a401d8e624eea4660b3f59\"\n",
        "os.environ[\"AZURE_OPENAI_API_VERSION\"] = api_version"
      ],
      "metadata": {
        "id": "tgiqM2EKMJjB"
      },
      "id": "tgiqM2EKMJjB",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ebUJkUQPMChl"
      },
      "id": "ebUJkUQPMChl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4b47436a",
      "metadata": {
        "id": "4b47436a"
      },
      "source": [
        "# How to route between sub-chains\n",
        "\n",
        ":::info Prerequisites\n",
        "\n",
        "This guide assumes familiarity with the following concepts:\n",
        "- [LangChain Expression Language (LCEL)](/docs/concepts/#langchain-expression-language)\n",
        "- [Chaining runnables](/docs/how_to/sequence/)\n",
        "- [Configuring chain parameters at runtime](/docs/how_to/configure)\n",
        "- [Prompt templates](/docs/concepts/#prompt-templates)\n",
        "- [Chat Messages](/docs/concepts/#message-types)\n",
        "\n",
        ":::\n",
        "\n",
        "Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing can help provide structure and consistency around interactions with models by allowing you to define states and use information related to those states as context to model calls.\n",
        "\n",
        "There are two ways to perform routing:\n",
        "\n",
        "1. Conditionally return runnables from a [`RunnableLambda`](/docs/how_to/functions) (recommended)\n",
        "2. Using a `RunnableBranch` (legacy)\n",
        "\n",
        "We'll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c6edac",
      "metadata": {
        "id": "c1c6edac"
      },
      "source": [
        "## Example Setup\n",
        "First, let's create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a8a1967",
      "metadata": {
        "id": "8a8a1967",
        "outputId": "15045daf-0adf-4583-93c9-d8eb6116c93c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gemini'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "model = AzureChatOpenAI(model=model_name,temperature=0.5)\n",
        "\n",
        "\n",
        "chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Gemini`, or `Other`.\n",
        "\n",
        "Do not respond with more than one word.\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Classification:\"\"\"\n",
        "    )\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"how do I call Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ],
      "metadata": {
        "id": "rhpM0hSvPe4J",
        "outputId": "f31b9f26-d0b9-4cc9-f54b-171792b9f979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "rhpM0hSvPe4J",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"what are the models available with OpenAI API?\"})"
      ],
      "metadata": {
        "id": "KjaNGHrGPivP",
        "outputId": "d46f61e8-1a87-4415-c391-894be9037417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "KjaNGHrGPivP",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Other'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7655555f",
      "metadata": {
        "id": "7655555f"
      },
      "source": [
        "Now, let's create three sub chains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "89d7722d",
      "metadata": {
        "id": "89d7722d"
      },
      "outputs": [],
      "source": [
        "langchain_chain = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert in langchain. \\\n",
        "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
        "Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model\n",
        "\n",
        "\n",
        "\n",
        "gemini_chain = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert in Vertex AI Gemini Models. \\\n",
        "Always answer questions starting with \"As Sundar Pichai told me\". \\\n",
        "Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model\n",
        "\n",
        "\n",
        "general_chain = PromptTemplate.from_template(\n",
        "    \"\"\"Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8d042c",
      "metadata": {
        "id": "6d8d042c"
      },
      "source": [
        "## Using a custom function (Recommended)\n",
        "\n",
        "You can also use a custom function to route between different outputs. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "687492da",
      "metadata": {
        "id": "687492da"
      },
      "outputs": [],
      "source": [
        "def route(info):\n",
        "    if \"gemini\" in info[\"topic\"].lower():\n",
        "        return gemini_chain\n",
        "    elif \"langchain\" in info[\"topic\"].lower():\n",
        "        return langchain_chain\n",
        "    else:\n",
        "        return general_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "02a33c86",
      "metadata": {
        "id": "02a33c86"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
        "    route\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c2e977a4",
      "metadata": {
        "id": "c2e977a4",
        "outputId": "9266bcae-60b3-4dc9-a6a8-1a8a88a45bb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Sundar Pichai told me, to use Gemini models in Vertex AI, you first need to ensure you have access to Google Cloud and have set up Vertex AI. Once you have that in place, you can start by creating a new project or using an existing one. Here are the steps to use Gemini models:\\n\\n1. **Set Up Your Environment**: Install the necessary SDKs and tools. For Python, you can use the Google Cloud SDK and Vertex AI SDK.\\n   \\n   ```bash\\n   pip install google-cloud-aiplatform\\n   ```\\n\\n2. **Authenticate**: Make sure you authenticate your environment to access Google Cloud services.\\n\\n   ```python\\n   from google.colab import auth\\n   auth.authenticate_user()\\n   ```\\n\\n3. **Initialize Vertex AI**: Set up your project and location.\\n\\n   ```python\\n   from google.cloud import aiplatform\\n\\n   aiplatform.init(project='your-project-id', location='your-region')\\n   ```\\n\\n4. **Create or Select a Dataset**: Use an existing dataset or create a new one for training your model.\\n\\n5. **Train Your Model**: Use the Vertex AI platform to train your Gemini model. You can do this through the Vertex AI console or programmatically using the SDK.\\n\\n   ```python\\n   job = aiplatform.CustomTrainingJob(\\n       display_name='gemini-training-job',\\n       script_path='training_script.py',\\n       container_uri='gcr.io/cloud-aiplatform/training/tf-cpu.2-3:latest',\\n       requirements=['tensorflow']\\n   )\\n\\n   job.run(\\n       dataset='your-dataset-id',\\n       model_display_name='gemini-model',\\n       args={'epochs': 10}\\n   )\\n   ```\\n\\n6. **Deploy Your Model**: Once trained, you can deploy the model to an endpoint for serving predictions.\\n\\n   ```python\\n   endpoint = aiplatform.Endpoint.create(display_name='gemini-endpoint')\\n   model = aiplatform.Model('your-model-id')\\n   model.deploy(endpoint=endpoint)\\n   ```\\n\\n7. **Make Predictions**: Use the deployed model to make predictions.\\n\\n   ```python\\n   predictions = endpoint.predict(instances=[{'input': 'your-input-data'}])\\n   print(predictions)\\n   ```\\n\\nBy following these steps, you can effectively use Gemini models in Vertex AI to train, deploy, and make predictions.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 486, 'prompt_tokens': 48, 'total_tokens': 534, 'completion_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_80a1bad4c7', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-7e402e83-f701-436d-ae83-716989122854-0', usage_metadata={'input_tokens': 48, 'output_tokens': 486, 'total_tokens': 534})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "48913dc6",
      "metadata": {
        "id": "48913dc6",
        "outputId": "a252aa59-cf39-467c-8780-2027e04ee7a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='As Harrison Chase told me, to use LangChain, you typically start by installing the necessary packages via pip. Once installed, you can begin by importing the relevant modules and setting up a language model. LangChain provides a variety of tools and utilities to facilitate the creation of language-based applications, such as text generation, question answering, and more. You can chain together different components, like prompt templates, memory, and various chains, to create complex workflows. For more detailed guidance, you might want to consult the official LangChain documentation or explore some example projects to see how everything fits together.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 44, 'total_tokens': 162, 'completion_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_80a1bad4c7', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-329ed11f-bfcb-4f49-bdbe-d2dc60ffa604-0', usage_metadata={'input_tokens': 44, 'output_tokens': 118, 'total_tokens': 162})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a14d0dca",
      "metadata": {
        "id": "a14d0dca",
        "outputId": "2a34325b-0e14-47be-dd28-b0d485f60df8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='2 + 2 equals 4.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 24, 'total_tokens': 32, 'completion_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_80a1bad4c7', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-fa164a83-e659-4234-8265-39f73306d265-0', usage_metadata={'input_tokens': 24, 'output_tokens': 8, 'total_tokens': 32})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5147b827",
      "metadata": {
        "id": "5147b827"
      },
      "source": [
        "## Using a RunnableBranch\n",
        "\n",
        "A `RunnableBranch` is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does **not** offer anything that you can't achieve in a custom function as described above, so we recommend using a custom function instead.\n",
        "\n",
        "A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n",
        "\n",
        "If no provided conditions match, it runs the default runnable.\n",
        "\n",
        "Here's an example of what it looks like in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2a101418",
      "metadata": {
        "id": "2a101418",
        "outputId": "ef32ec47-f55b-4863-ec7b-04597cfff397",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Sundar Pichai told me, to use Gemini, you would start by accessing Google Cloud's Vertex AI platform. Gemini models are part of the advanced AI offerings within Vertex AI, designed to provide powerful machine learning capabilities. Here’s a step-by-step guide to get you started:\\n\\n1. **Set Up Your Environment**: Ensure you have a Google Cloud account and the necessary permissions to access Vertex AI services. Install the Google Cloud SDK and authenticate your environment.\\n\\n2. **Create a Vertex AI Workspace**: Navigate to the Vertex AI section in the Google Cloud Console and create a new workspace for your project.\\n\\n3. **Select a Gemini Model**: Within the Vertex AI interface, you can browse the available models. Choose a Gemini model that fits your use case, whether it's for natural language processing, computer vision, or another application.\\n\\n4. **Upload Your Data**: Prepare and upload your dataset to Google Cloud Storage. Ensure your data is properly formatted and preprocessed according to the requirements of the Gemini model you have chosen.\\n\\n5. **Train Your Model**: Use the Vertex AI training interface to configure and start the training process. You can specify various parameters and hyperparameters to optimize the performance of your Gemini model.\\n\\n6. **Evaluate and Tune**: After training, evaluate your model's performance using the provided metrics and tools. Fine-tune the model as necessary by adjusting parameters and retraining.\\n\\n7. **Deploy Your Model**: Once satisfied with the performance, deploy your model using Vertex AI’s deployment tools. This will make your model accessible via an API endpoint, which you can integrate into your applications.\\n\\n8. **Monitor and Maintain**: After deployment, continuously monitor the performance of your model and make adjustments as needed to ensure it remains effective and accurate.\\n\\nBy following these steps, you can effectively leverage the power of Gemini models within Vertex AI to build and deploy sophisticated machine learning solutions.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 385, 'prompt_tokens': 48, 'total_tokens': 433, 'completion_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_80a1bad4c7', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-ef218064-0514-43f6-a01f-2b4ccf21db76-0', usage_metadata={'input_tokens': 48, 'output_tokens': 385, 'total_tokens': 433})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableBranch\n",
        "\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"gemini\" in x[\"topic\"].lower(), gemini_chain),\n",
        "    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
        "    general_chain,\n",
        ")\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch\n",
        "full_chain.invoke({\"question\": \"how do I use Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8d8caf9b",
      "metadata": {
        "id": "8d8caf9b",
        "outputId": "9283872d-8b0a-4608-93b9-0e3294977810",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='As Harrison Chase told me, using LangChain involves a few key steps. First, you need to install the LangChain library using pip:\\n\\n```bash\\npip install langchain\\n```\\n\\nOnce installed, you can start by importing the necessary modules and setting up the chain. Here\\'s a simple example to get you started:\\n\\n```python\\nfrom langchain import LangChain\\n\\n# Initialize the LangChain object\\nchain = LangChain()\\n\\n# Define your prompt or input text\\nprompt = \"Translate the following English text to French: \\'Hello, how are you?\\'\"\\n\\n# Use the chain to process the prompt\\nresponse = chain.process(prompt)\\n\\n# Print the response\\nprint(response)\\n```\\n\\nThis is a basic example, but LangChain offers a lot of flexibility and advanced features, such as integrating with different models, customizing prompts, and handling more complex workflows. For more detailed examples and documentation, you can visit the official LangChain GitHub repository or their documentation site.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens': 44, 'total_tokens': 238, 'completion_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_80a1bad4c7', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-184b4114-786a-458c-8d85-a30f8f68facb-0', usage_metadata={'input_tokens': 44, 'output_tokens': 194, 'total_tokens': 238})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "26159af7",
      "metadata": {
        "id": "26159af7",
        "outputId": "94f52bca-8605-430b-d0f5-d5d088c17797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='2 + 2 equals 4.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 24, 'total_tokens': 32, 'completion_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_80a1bad4c7', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-4e36532d-a634-48ec-b38a-9a616978a875-0', usage_metadata={'input_tokens': 24, 'output_tokens': 8, 'total_tokens': 32})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff40bcb3",
      "metadata": {
        "id": "ff40bcb3"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "You've now learned how to add routing to your composed LCEL chains.\n",
        "\n",
        "Next, check out the other how-to guides on runnables in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927b7498",
      "metadata": {
        "id": "927b7498"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}