{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-for-Professionals/blob/main/EY2024/Additional/Prompt_Engineering_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rn1lLJXRfSZ"
      },
      "source": [
        "# Prompt Engineering Techniques\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "! pip3 install --user --quiet openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPFAnv3rM8U0"
      },
      "outputs": [],
      "source": [
        "api_key = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "api_version = \"2023-07-01-preview\" # \"2023-05-15\", 2024-02-01\n",
        "azure_endpoint = \"https://xxxxxxxxx.openai.azure.com/\"\n",
        "deployment_name = \"xxxxxxxxxxxx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWcDdeofM8U0"
      },
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "\n",
        "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
        "client = AzureOpenAI(\n",
        "    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
        "    api_version=api_version,\n",
        "    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
        "    azure_endpoint=azure_endpoint,\n",
        "    api_key = api_key,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv50i87gy_UX"
      },
      "outputs": [],
      "source": [
        "def generate_response(prompt,temp=0.0):\n",
        "  response = client.chat.completions.create(\n",
        "      messages=[{\"role\":\"system\",'content':\"You are an expert programmer, you follow standard best practices for answering coding questions.\"},\n",
        "            {\"role\":\"user\",'content':prompt}],\n",
        "      model = deployment_name,\n",
        "      temperature=temp,\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Gemini 1.5 Flash model for these given below prompting techniques\n",
        "\n",
        "### There are different prompting techniques:\n",
        "**1) Multi-turn conversation or Chatbot with conversation history** <br>\n",
        "**2) Zero-shot Prompting** <br>\n",
        "**3) Few-shot Prompting** <br>\n",
        "**4) Chain-of-Thought Prompting** <br>\n",
        "**5) Tree-of-Thought Prompting** <br>\n",
        "**6) ReAct Prompting(Reason and Act)** <br>"
      ],
      "metadata": {
        "id": "26TpFzFpFPSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-turn Conversation**"
      ],
      "metadata": {
        "id": "X8VOeA5Ahta3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\n",
        "\n",
        "while True:\n",
        "  user = input(\"You: \")\n",
        "  if user == \"quit\":\n",
        "    break\n",
        "  prompt = prompt + \"User: \" + user + \"Assistant : \"\n",
        "  ans=generate_response(prompt)\n",
        "  print(\"Assistant: \", ans)\n",
        "  prompt = prompt + ans + \"\\n\"\n"
      ],
      "metadata": {
        "id": "7nzs8Zifhsr-",
        "outputId": "626965ef-bb11-47f5-e158-2866686d2fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: hi\n",
            "Assistant:  Hi! üëã  How can I help you today? üòä \n",
            "\n",
            "You: how are you?\n",
            "Assistant:  I'm doing well, thank you for asking!  How can I help you today? üòä \n",
            "\n",
            "You: I want to learn Generative AI\n",
            "Assistant:  That's great! Generative AI is a fascinating field. What specifically interests you about it?  \n",
            "\n",
            "For example, do you want to:\n",
            "* **Understand the basics?** \n",
            "* **Learn about specific generative AI models like DALL-E or ChatGPT?**\n",
            "* **Explore its applications in different industries?**\n",
            "\n",
            "Let me know, and I can provide you with information or resources to get you started! \n",
            "\n",
            "You: Who is Elon Musk?\n",
            "Assistant:  Elon Musk is a business magnate, entrepreneur, and investor. He is the founder, CEO, and Chief Engineer of SpaceX; early investor, CEO, and Product Architect of Tesla, Inc.; founder of The Boring Company; and co-founder of Neuralink and OpenAI. \n",
            "\n",
            "You: What was the topic, that I wanted to learn?\n",
            "Assistant:  You wanted to learn about **Generative AI**. \n",
            "\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero-shot prompting**\n",
        "\n",
        "Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it."
      ],
      "metadata": {
        "id": "IV76I9qwIQeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''\n",
        "Message: Hi alex, thanks for the thoughtful birthday card!\n",
        "Sentiment: ?\n",
        "'''\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGtltyWkH5Hu",
        "outputId": "15d9bd3e-5ec3-4a9c-f1df-724983198870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appreciation \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Few-shot prompting**\n",
        "\n",
        "Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response."
      ],
      "metadata": {
        "id": "uJz9R8erKb3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
        "Sentiment: Negative\n",
        "\n",
        "Message: Can't wait to order pizza for dinner tonight\n",
        "Sentiment: Positive\n",
        "\n",
        "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
        "Sentiment: ?\n",
        "\"\"\"\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfn3Zh3HJnBQ",
        "outputId": "819b99a1-0f13-4767-8ced-8fd4b25c108f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Positive \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specifying the Output Format\n",
        "- You can also specify the format in which you want the model to respond.\n",
        "- In the example below, you are asking to \"give a one word response\"."
      ],
      "metadata": {
        "id": "J3F2X283K2nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
        "Sentiment: Negative\n",
        "\n",
        "Message: Can't wait to order pizza for dinner tonight\n",
        "Sentiment: Positive\n",
        "\n",
        "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
        "Sentiment: ?\n",
        "\n",
        "Give a one word response.\n",
        "\"\"\"\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E26iKMrKYK0",
        "outputId": "35a7b6ab-a558-4133-cb8a-e4ba2b194468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chain-of-Thought Prompting**\n",
        "chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
      ],
      "metadata": {
        "id": "6z21K3uzVN8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPE0hYEUMGkX",
        "outputId": "312518c5-eb62-4167-cfcf-09e9421d981d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tree of Thoughts (ToT)**\n",
        "\n",
        "Tree of Thoughts (ToT) prompting is a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that serve as intermediate steps for general problem-solving with language models.\n",
        "\n",
        "\n",
        "How does it work?\n",
        "\n",
        "- ToT prompting breaks problems down into smaller parts, similar to CoT prompting, but goes further by combining this with the ability to explore multiple solution paths in parallel, forming a tree instead of a single chain. Each thought is generated or solved independently and passed to the next step, allowing the model to self-evaluate and decide whether to continue with that path or choose another."
      ],
      "metadata": {
        "id": "bQvoNRxqVo5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"\"\"\n",
        "Imagine 5 different experts are answering this question.\n",
        "They will brainstorm the answer step by step reasoning carefully and taking all facts into consideration\n",
        "All experts will write down 1 step of their thinking,\n",
        "then share it with the group.\n",
        "They will each critique their response, and the all the responses of others\n",
        "They will check their answer based on science and the laws of physics\n",
        "They will keep going through steps until they reach their conclusion taking into account the thoughts of the other experts\n",
        "If at any time they realise that there is a flaw in their logic they will backtrack to where that flaw occurred\n",
        "If any expert realises they're wrong at any point then they acknowledges this and start another train of thought\n",
        "Each expert will assign a likelihood of their current assertion being correct\n",
        "Continue until the experts agree on the single most likely location\n",
        "The question is.\n",
        "1. Carlos is at the swimming pool.\n",
        "2. He walks to the locker room, carrying a towel.\n",
        "3. He puts his watch in the towel and carries the towel tightly to a lounger at the poolside.\n",
        "4. At the lounger he opens and vigorously shakes the towel, then walks to the snack bar.\n",
        "5. He leaves the towel at the snack bar, then walks to the diving board.\n",
        "6. Later Carlos realises he has has lost his watch. Where is the single most likely location of the watch?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF7_WBM9MT78",
        "outputId": "48e7432f-5d44-411b-e0a9-0d08471b52ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a breakdown of how the experts might reason through this:\n",
            "\n",
            "**Expert 1:**  The watch was last seen in the towel.  The towel was left at the snack bar.  Therefore, the watch is most likely at the snack bar. (Likelihood: 80%)\n",
            "\n",
            "**Expert 2:**  We need to consider all possibilities. The watch could have fallen out of the towel while Carlos was shaking it.  This means the watch could be near the lounger. (Likelihood: 20%)\n",
            "\n",
            "**Expert 3:**  While it's possible the watch fell out during the shaking, the towel was tightly wrapped around the watch, making that unlikely. The watch was also likely secured within the towel. (Likelihood: 90%) \n",
            "\n",
            "**Expert 4:**  Expert 3 is right, the watch was secure.  It's much more likely that the watch is at the snack bar with the towel. (Likelihood: 95%)\n",
            "\n",
            "**Expert 5:**  We need to consider the possibility of theft.  While unlikely, someone could have taken the watch from the towel at the snack bar. (Likelihood: 5%)\n",
            "\n",
            "**Group Discussion:**\n",
            "\n",
            "* The experts agree that the watch was last seen in the towel.\n",
            "* The experts agree that the most likely location for the towel is at the snack bar. \n",
            "* The experts acknowledge the possibility of the watch falling out during the shaking, but deem it unlikely due to the way the towel was wrapped. \n",
            "* The experts acknowledge the possibility of theft, but deem it less likely given the towel's location.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "The single most likely location of the watch is **at the snack bar, inside the towel**. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReAct Prompting**\n",
        "\n",
        "ReAct is a framework where LLMs are used to generate both reasoning traces and task-specific actions in an interleaved manner.\n",
        "\n",
        "Generating reasoning traces allow the model to induce, track, and update action plans, and even handle exceptions. The action step allows to interface with and gather information from external sources such as knowledge bases or environments."
      ],
      "metadata": {
        "id": "i3Sfro9fQ1sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Solve a question answering task with interleaving\n",
        "Thought,\n",
        "Action,\n",
        "Observation steps.\n",
        "\n",
        "Thought can reason about the current situation, and Action can be three types:\n",
        "\n",
        "Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.\n",
        "\n",
        "Calculator[query], it performs mathematical calculations based on input query.\n",
        "\n",
        "Lookup[keyword], which returns the next sentence containing keyword in the current passage.\n",
        "\n",
        "Finish[answer], which returns the answer and finishes the task.\n",
        "\n",
        "Question:\n",
        "\n",
        "Who is Elon Musk?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u1q_MyZO68A",
        "outputId": "21ff22b2-77b2-4437-bcbc-ffd73dc1f42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to find information about Elon Musk.\n",
            "Action: Search[Elon Musk]\n",
            "Observation: Elon Reeve Musk (born June 28, 1971) is a business magnate and entrepreneur. He is the founder, CEO and Chief Engineer of SpaceX; early investor, CEO and Product Architect of Tesla, Inc.; founder of The Boring Company; and co-founder of Neuralink and OpenAI. He is the world's second-wealthiest person, according to the Bloomberg Billionaires Index and Forbes's real-time billionaire list, as of December 2022.\n",
            "Thought: I have the information about Elon Musk.\n",
            "Action: Finish[Elon Musk is a business magnate and entrepreneur. He is the founder, CEO and Chief Engineer of SpaceX; early investor, CEO and Product Architect of Tesla, Inc.; founder of The Boring Company; and co-founder of Neuralink and OpenAI.] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Chain-of-Verification Prompting**\n",
        "\n",
        "The Chain-of-Verification (CoVe) prompt engineering method aims to reduce hallucinations through a verification loop."
      ],
      "metadata": {
        "id": "dPnsQc3hpQ7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question=\"Name some athletes who were born in United states\"\n",
        "\n",
        "prompt=f'''Here is the question: {Question}.\n",
        "\n",
        "First, generate a response.\n",
        "\n",
        "Then, create and answer verification questions based on this response to check for accuracy. Think it through and make sure you are extremely accurate based on the question asked.\n",
        "\n",
        "After answering each verification question, consider these answers and revise the initial response to formulate a final, verified answer. Ensure the final response reflects the accuracy and findings from the verification process.'''\n",
        "\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9mjZtAJfDdm",
        "outputId": "697f1f29-7a6e-4d72-cf1b-a4c32b3d0e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Initial Response:\n",
            "\n",
            "Some athletes born in the United States include Michael Jordan, Serena Williams, Tom Brady, and Lebron James.\n",
            "\n",
            "## Verification Questions:\n",
            "\n",
            "1. **Are all the athletes listed born in the United States?** \n",
            "2. **Are all the athletes listed considered \"athletes\" in the traditional sense?** \n",
            "3. **Are there any other famous athletes born in the United States that should be included in the list?**\n",
            "\n",
            "## Answering Verification Questions:\n",
            "\n",
            "1. **Yes.**  All the athletes listed were born in the United States.\n",
            "2. **Yes.**  All listed individuals are considered athletes in the traditional sense.\n",
            "3. **Yes.**  There are many other famous athletes born in the United States, including  Muhammad Ali, Tiger Woods, and Simone Biles.\n",
            "\n",
            "## Final, Verified Response:\n",
            "\n",
            "Some athletes born in the United States include Michael Jordan, Serena Williams, Tom Brady, Lebron James, Muhammad Ali, Tiger Woods, and Simone Biles. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Step-Back Prompting**\n",
        "\n",
        "A crucial rule to remember when prompting LLMs is to give them space to 'think'. You don‚Äôt want to overly constrain the model such that it can‚Äôt explore various solutions.\n",
        "\n",
        "Chain of thought reasoning is one way to push the model to think through the problem. A simple way to implement this type of reasoning is to add a statement like ‚Äúthink through this task step-by-step‚Äù at the end of your prompt."
      ],
      "metadata": {
        "id": "YEF7Xk88tIQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question=\"What is Quantum Physics?\"\n",
        "prompt=f'''Here is a question or task: {Question}\n",
        "\n",
        "Let's think step-by-step to answer this:\n",
        "\n",
        "Step 1) Abstract the key concepts and principles relevant to this question:\n",
        "\n",
        "Step 2) Use the abstractions to reason through the question:\n",
        "\n",
        "Final Answer: '''\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqmZDv7ApGI6",
        "outputId": "e63c2e49-80fd-41da-862d-6c8a988fa1f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Step 1: Key Concepts and Principles\n",
            "\n",
            "* **Quantum Mechanics:**  A fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles.\n",
            "* **Quantization:** The idea that certain physical quantities, like energy, can only exist in discrete values (quanta) rather than continuously.\n",
            "* **Wave-Particle Duality:**  The concept that particles, like electrons, exhibit both wave-like and particle-like properties.\n",
            "* **Uncertainty Principle:**  States that it is impossible to know both the position and momentum of a particle with absolute certainty.\n",
            "* **Superposition:**  A state where a quantum system can exist in multiple states simultaneously until measured.\n",
            "* **Entanglement:**  A phenomenon where two or more quantum particles become interconnected, even when separated by vast distances.\n",
            "\n",
            "## Step 2: Reasoning\n",
            "\n",
            "* Quantum physics studies the behavior of matter and energy at the atomic and subatomic levels.\n",
            "* Unlike classical physics, which assumes a smooth and continuous world, quantum physics reveals a fundamentally discrete and probabilistic nature.\n",
            "* The behavior of particles at this scale is governed by unique principles like quantization, wave-particle duality, and the uncertainty principle.\n",
            "* Quantum phenomena, like superposition and entanglement, lead to seemingly counterintuitive but experimentally verified results.\n",
            "\n",
            "## Final Answer:\n",
            "\n",
            "Quantum physics is a branch of physics that explores the nature of reality at the atomic and subatomic level. It reveals that the world at this scale behaves in a fundamentally different way from our everyday experience, characterized by quantization, wave-particle duality, and probabilistic outcomes. These principles lead to phenomena like superposition and entanglement, which have significant implications for our understanding of the universe and have the potential for transformative technologies. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UfoRBs-os3rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thank You"
      ],
      "metadata": {
        "id": "23-8_wu6QI-j"
      }
    }
  ]
}