{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMo3j+6ikwHdaej7J2gmzUQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-for-Professionals/blob/main/EY2024/C11_LLM_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge-score --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ1SBbG5xpjw",
        "outputId": "bf56727d-74ce-4ccc-990d-3a9ac90126c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics for Language Models\n",
        "\n",
        "## Perplexity\n",
        "\n",
        "**Definition**: Perplexity is a measure of how well a language model predicts a sample. A lower perplexity indicates that the model is better at predicting the sample.\n",
        "\n",
        "**Interpretation**:\n",
        "- **Low Perplexity**: Indicates better performance (e.g., 10 or lower).\n",
        "- **High Perplexity**: Indicates worse performance.\n",
        "\n",
        "**Benchmark**: For modern language models, perplexity values typically range between 10 and 50 for well-formed English text.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uxVPzUNxz9lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "id": "_O2hs1MLzSsd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "predictions = [\"the cat is on the the mat in house in the city\", \"the dog is running towards cat\"]\n",
        "references = [[\"the cat is on the the mat in city\"], [\"the dog is running towards cat\"]]"
      ],
      "metadata": {
        "id": "PxgV9Ug2zU9Q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "YvxunFyj1swI"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_word_probabilities(prediction, reference):\n",
        "    prediction_tokens = prediction.split()\n",
        "    reference_tokens = reference.split()\n",
        "\n",
        "    # Count word frequencies in the prediction\n",
        "    prediction_counts = Counter(prediction_tokens)\n",
        "    total_words = sum(prediction_counts.values())\n",
        "\n",
        "    # Calculate probability of each reference word being in the prediction\n",
        "    probabilities = []\n",
        "    for word in reference_tokens:\n",
        "        # Frequency of word in prediction / total words in prediction\n",
        "        probabilities.append(prediction_counts.get(word, 0) / total_words)\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def calculate_perplexity(probabilities):\n",
        "    N = len(probabilities)  # Total number of words (or tokens)\n",
        "\n",
        "    # Ensure no probability is zero to avoid log(0) issue\n",
        "    probabilities = [p if p > 0 else 1e-10 for p in probabilities]\n",
        "\n",
        "    log_sum = sum([math.log2(p) for p in probabilities])\n",
        "    perplexity = 2 ** (-1/N * log_sum)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "perplexities = [calculate_perplexity(calculate_word_probabilities(pred, ref[0])) for pred, ref in zip(predictions, references)]\n",
        "average_perplexity = sum(perplexities) / len(perplexities)\n",
        "print(f\"Average Perplexity: {average_perplexity}\")"
      ],
      "metadata": {
        "id": "_ZGKI4P81hOu",
        "outputId": "7e7b7fc3-7cba-4aa3-c3fd-d7abb9ceeaf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Perplexity: 6.499587118728348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "predictions = [\"the cat is on the mat\", \"there is a cat on the the mat in the garage\"]\n",
        "references = [[\"the cat is on the mat\"], [\"there is a cat on the mat\"]]\n",
        "\n",
        "perplexities = [calculate_perplexity(calculate_word_probabilities(pred, ref[0])) for pred, ref in zip(predictions, references)]\n",
        "average_perplexity = sum(perplexities) / len(perplexities)\n",
        "print(f\"Average Perplexity: {average_perplexity}\")"
      ],
      "metadata": {
        "id": "3qxMPFKJD-wv",
        "outputId": "9bf4198b-1ea3-4a07-f909-fc00d9f83a59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Perplexity: 7.082234277441635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU (Bilingual Evaluation Understudy)\n",
        "\n",
        "**Definition**: BLEU is a metric for evaluating a generated sentence to a reference sentence. It measures the n-gram precision with a penalty for overly short sentences.\n",
        "\n",
        "**Interpretation**:\n",
        "- **High BLEU Score**: Indicates good performance (e.g., scores above 0.5 or 50%).\n",
        "- **Low BLEU Score**: Indicates poor performance.\n",
        "\n",
        "**Benchmark**: For machine translation tasks, a BLEU score above 0.3 (30%) is considered reasonable, while scores above 0.5 (50%) are considered good.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "L8ce5bPbUqgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU Score Calculation\n",
        "def calculate_bleu(predicted_sentence, reference_sentence):\n",
        "    return sentence_bleu([reference_sentence.split()], predicted_sentence.split())\n",
        "\n",
        "bleu_scores = [calculate_bleu(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(f\"Average BLEU Score: {average_bleu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU-Bq3e0zjfL",
        "outputId": "dce18732-fa29-4b13-e391-d1e5024a60be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 0.751128696864156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
        "\n",
        "**Definition**: ROUGE measures the overlap of n-grams between the generated sentence and the reference sentence, focusing on recall.\n",
        "\n",
        "**Types**:\n",
        "- **ROUGE-1**: Measures the overlap of unigrams.\n",
        "- **ROUGE-2**: Measures the overlap of bigrams.\n",
        "- **ROUGE-L**: Measures the longest common subsequence.\n",
        "\n",
        "**Interpretation**:\n",
        "- **High ROUGE Score**: Indicates good performance.\n",
        "- **Low ROUGE Score**: Indicates poor performance.\n",
        "\n",
        "**Benchmark**: For summarization tasks, ROUGE scores of 0.5 (50%) or higher are considered good.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dWBJ4fIvUuTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE Score Calculation\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def calculate_rouge(predicted_sentence, reference_sentence):\n",
        "    scores = scorer.score(reference_sentence, predicted_sentence)\n",
        "    return scores\n",
        "\n",
        "rouge_scores = [calculate_rouge(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "\n",
        "average_rouge = {\n",
        "    'rouge1': sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rouge2': sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rougeL': sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "}\n",
        "\n",
        "print(f\" Average ROUGE Scores: {average_rouge}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSMlLsJazmDh",
        "outputId": "eb36217d-cd3b-4529-d34c-3b1f36c53372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Average ROUGE Scores: {'rouge1': 0.8888888888888888, 'rouge2': 0.875, 'rougeL': 0.8888888888888888}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "inputs = [\n",
        "    \"Translate the following English text to French: 'Hello, how are you?'\",\n",
        "    \"Summarize the following text wihtout loosing context: 'The quick brown fox jumps over the lazy dog.'\"\n",
        "]\n",
        "references = [\n",
        "    [\"Bonjour, comment ça va?\"],\n",
        "    [\"The quick brown fox jumps over the lazy dog.\"]\n",
        "]"
      ],
      "metadata": {
        "id": "R-7XSNqdxoh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DOXSfnNz2Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "outputId": "4d8b813e-6c97-4f5c-8995-9a6b3bdcd7ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/5.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/5.1 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip3 install --upgrade --user --quiet openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"xxxxxxxxxxxxxxxx\"\n",
        "api_version = \"2023-07-01-preview\" # \"2023-05-15\"\n",
        "azure_endpoint = \"https://xxxxxxxxx.openai.azure.com/\"\n",
        "model_name = \"gpt-4o\"\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
        "client = AzureOpenAI(\n",
        "    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
        "    api_version=api_version,\n",
        "    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
        "    azure_endpoint=azure_endpoint,\n",
        "    api_key = api_key,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "ksX0sguJZK0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lslYAvw37JGQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_response(prompt,temp=0.0):\n",
        "  response = client.chat.completions.create(\n",
        "      messages=[{\"role\":\"system\",'content':\"You are an expert programmer, you follow standard best practices for answering coding questions.\"},\n",
        "            {\"role\":\"user\",'content':prompt}],\n",
        "      model = model_name,\n",
        "      temperature=temp,\n",
        "  )\n",
        "  return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from Gemini\n",
        "predictions = [generate_response(input_text) for input_text in inputs]"
      ],
      "metadata": {
        "id": "AAtsTC5pzHji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(references)):\n",
        "  print(f\"Reference {i+1}: {references[i][0]}\")\n",
        "  print(f\"Prediction {i+1}: {predictions[i]}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "Jmpe2kayIbW_",
        "outputId": "15f46910-17fd-4a96-bba7-1be6598e10eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1: Bonjour, comment ça va?\n",
            "Prediction 1: Bonjour, comment allez-vous ? \n",
            "\n",
            "\n",
            "Reference 2: The quick brown fox jumps over the lazy dog.\n",
            "Prediction 2: A swift, brown fox leaps over a lethargic dog. \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity\n",
        "perplexities = [calculate_perplexity(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_perplexity = sum(perplexities) / len(perplexities)\n",
        "print(f\"Average Perplexity: {average_perplexity}\")"
      ],
      "metadata": {
        "id": "tzkDA9FTz3TX",
        "outputId": "87383dea-663c-4bdd-d824-390825baeb85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Perplexity: 0.8916905184686361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # BLEU Score Calculation\n",
        "bleu_scores = [calculate_bleu(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(f\"Average BLEU Score: {average_bleu}\")"
      ],
      "metadata": {
        "id": "F5_zOrk4IZwt",
        "outputId": "dac4805b-27c4-4a4f-e2d3-94f0d4fad6b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 8.386418434906823e-155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE Score\n",
        "rouge_scores = [calculate_rouge(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_rouge = {\n",
        "    'rouge1': sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rouge2': sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rougeL': sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "}\n",
        "print(f\"ROUGE 1 Score: {average_rouge['rouge1']}\")\n",
        "print(f\"ROUGE 2 Score: {average_rouge['rouge2']}\")\n",
        "print(f\"ROUGE L Score: {average_rouge['rougeL']}\")"
      ],
      "metadata": {
        "id": "15tm0vg9IutO",
        "outputId": "29596ae2-f291-473b-ae70-8422f6d24c10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE 1 Score: 0.4722222222222222\n",
            "ROUGE 2 Score: 0.22916666666666666\n",
            "ROUGE L Score: 0.4722222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_oRTiqyJNLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "Here’s a quick summary of the metrics:\n",
        "\n",
        "| Metric      | Definition                                                                                       | Interpretation                                   | Benchmark                                 |\n",
        "|-------------|--------------------------------------------------------------------------------------------------|-------------------------------------------------|-------------------------------------------|\n",
        "| **Perplexity** | Measures how well a model predicts a sample. Lower values are better.                            | Low = Better (e.g., 10 or lower), High = Worse  | 10-50 for well-formed English text         |\n",
        "| **BLEU**      | Evaluates generated sentence against reference. Measures n-gram precision with penalty for short sentences. | High = Better (e.g., > 0.5), Low = Worse         | > 0.3 is reasonable, > 0.5 is good         |\n",
        "| **ROUGE**     | Measures n-gram overlap between generated and reference sentences. Focuses on recall.            | High = Better (e.g., > 0.5), Low = Worse         | > 0.5 for summarization tasks              |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bWeTI6RZUy0m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FC6S0HKGU0rv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}