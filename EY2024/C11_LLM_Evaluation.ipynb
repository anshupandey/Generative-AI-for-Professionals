{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLJq512CnTVn8AIzhDfZp/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-for-Professionals/blob/main/EY2024/C11_LLM_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge-score --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ1SBbG5xpjw",
        "outputId": "bf56727d-74ce-4ccc-990d-3a9ac90126c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics for Language Models\n",
        "\n",
        "## Perplexity\n",
        "\n",
        "**Definition**: Perplexity is a measure of how well a language model predicts a sample. A lower perplexity indicates that the model is better at predicting the sample.\n",
        "\n",
        "**Interpretation**:\n",
        "- **Low Perplexity**: Indicates better performance (e.g., 10 or lower).\n",
        "- **High Perplexity**: Indicates worse performance.\n",
        "\n",
        "**Benchmark**: For modern language models, perplexity values typically range between 10 and 50 for well-formed English text.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uxVPzUNxz9lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "id": "_O2hs1MLzSsd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "predictions = [\"the cat is on the the mat in house in the city\", \"the dog is running towards cat\"]\n",
        "references = [[\"the cat is on the the mat in city\"], [\"the dog is running towards cat\"]]"
      ],
      "metadata": {
        "id": "PxgV9Ug2zU9Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "YvxunFyj1swI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_word_probabilities(prediction, reference):\n",
        "    prediction_tokens = prediction.split()\n",
        "    reference_tokens = reference.split()\n",
        "\n",
        "    # Count word frequencies in the prediction\n",
        "    prediction_counts = Counter(prediction_tokens)\n",
        "    total_words = sum(prediction_counts.values())\n",
        "\n",
        "    # Calculate probability of each reference word being in the prediction\n",
        "    probabilities = []\n",
        "    for word in reference_tokens:\n",
        "        # Frequency of word in prediction / total words in prediction\n",
        "        probabilities.append(prediction_counts.get(word, 0) / total_words)\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def calculate_perplexity(probabilities):\n",
        "    N = len(probabilities)  # Total number of words (or tokens)\n",
        "\n",
        "    # Ensure no probability is zero to avoid log(0) issue\n",
        "    probabilities = [p if p > 0 else 1e-10 for p in probabilities]\n",
        "\n",
        "    log_sum = sum([math.log2(p) for p in probabilities])\n",
        "    perplexity = 2 ** (-1/N * log_sum)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "perplexities = [calculate_perplexity(calculate_word_probabilities(pred, ref[0])) for pred, ref in zip(predictions, references)]\n",
        "average_perplexity = sum(perplexities) / len(perplexities)\n",
        "print(f\"Average Perplexity: {average_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZGKI4P81hOu",
        "outputId": "ae27edcb-6847-4a4d-8123-9a83914d976f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Perplexity: 6.499587118728348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "predictions = [\"the cat is on the mat\", \"there is a cat on the the mat in the garage moving outwards towards tree\"]\n",
        "references = [[\"the cat is on the mat\"], [\"there is a cat on the mat\"]]\n",
        "\n",
        "perplexities = [calculate_perplexity(calculate_word_probabilities(pred, ref[0])) for pred, ref in zip(predictions, references)]\n",
        "average_perplexity = sum(perplexities) / len(perplexities)\n",
        "print(f\"Average Perplexity: {average_perplexity}\")"
      ],
      "metadata": {
        "id": "3qxMPFKJD-wv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c39bb69-2a6e-4540-f014-c4216b0c61bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Perplexity: 8.791737077255942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU (Bilingual Evaluation Understudy)\n",
        "\n",
        "**Definition**: BLEU is a metric for evaluating a generated sentence to a reference sentence. It measures the n-gram precision with a penalty for overly short sentences.\n",
        "\n",
        "**Interpretation**:\n",
        "- **High BLEU Score**: Indicates good performance (e.g., scores above 0.5 or 50%).\n",
        "- **Low BLEU Score**: Indicates poor performance.\n",
        "\n",
        "**Benchmark**: For machine translation tasks, a BLEU score above 0.3 (30%) is considered reasonable, while scores above 0.5 (50%) are considered good.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "L8ce5bPbUqgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "predictions = [\"the cat is on the mat\", \"there is a cat on the the mat in the garage moving outwards towards tree\"]\n",
        "references = [[\"the cat is on the mat\"], [\"there is a cat on the mat\"]]\n"
      ],
      "metadata": {
        "id": "kpg9yQxw5eGW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU Score Calculation\n",
        "def calculate_bleu(predicted_sentence, reference_sentence):\n",
        "    return sentence_bleu([reference_sentence.split()], predicted_sentence.split())\n",
        "\n",
        "bleu_scores = [calculate_bleu(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(f\"Average BLEU Score: {average_bleu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU-Bq3e0zjfL",
        "outputId": "50b9ac5c-cd07-4822-b9a2-c05af3db20eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 0.6760928267911618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "predictions = [\"the cat is on the mat\", \"there is a cat on the the mat in the garage moving outwards towards tree\"]\n",
        "references = [[\"the cat is on the mat in the city Bangalore\"], [\"there is a cat on the mat\"]]\n",
        "\n",
        "bleu_scores = [calculate_bleu(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(f\"Average BLEU Score: {average_bleu}\")"
      ],
      "metadata": {
        "id": "oQnBRMtj5gVl",
        "outputId": "96dab9e5-356b-45b3-9349-d35fb6d09f26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 0.4328013863074578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
        "\n",
        "**Definition**: ROUGE measures the overlap of n-grams between the generated sentence and the reference sentence, focusing on recall.\n",
        "\n",
        "**Types**:\n",
        "- **ROUGE-1**: Measures the overlap of unigrams.\n",
        "- **ROUGE-2**: Measures the overlap of bigrams.\n",
        "- **ROUGE-L**: Measures the longest common subsequence.\n",
        "\n",
        "**Interpretation**:\n",
        "- **High ROUGE Score**: Indicates good performance.\n",
        "- **Low ROUGE Score**: Indicates poor performance.\n",
        "\n",
        "**Benchmark**: For summarization tasks, ROUGE scores of 0.5 (50%) or higher are considered good.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dWBJ4fIvUuTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "predictions = [\"the cat is on the mat\", \"there is a cat on the the mat in the garage moving outwards towards tree\"]\n",
        "references = [[\"the cat is on the mat in the city Bangalore\"], [\"there is a cat on the mat\"]]\n"
      ],
      "metadata": {
        "id": "Px3D2kEm6YPi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE Score Calculation\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def calculate_rouge(predicted_sentence, reference_sentence):\n",
        "    scores = scorer.score(reference_sentence, predicted_sentence)\n",
        "    return scores\n",
        "\n",
        "rouge_scores = [calculate_rouge(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "\n",
        "average_rouge = {\n",
        "    'rouge1': sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rouge2': sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rougeL': sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "}\n",
        "\n",
        "print(f\" Average ROUGE Scores: {average_rouge}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSMlLsJazmDh",
        "outputId": "5b19abe2-b605-4697-b2d1-a4a20fb74926"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Average ROUGE Scores: {'rouge1': 0.6931818181818181, 'rouge2': 0.6571428571428571, 'rougeL': 0.6931818181818181}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "inputs = [\n",
        "    \"Translate the following English text to French: 'Hello, how are you?'\",\n",
        "    \"Summarize the following text wihtout loosing context: 'The quick brown fox jumps over the lazy dog.'\"\n",
        "]\n",
        "references = [\n",
        "    [\"Bonjour, comment ça va?\"],\n",
        "    [\"The quick brown fox jumps over the lazy dog.\"]\n",
        "]"
      ],
      "metadata": {
        "id": "R-7XSNqdxoh5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DOXSfnNz2Ke"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "!pip install --user --quiet openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"62d77bb382974118aec75d84d274bb72\"\n",
        "api_version = \"2024-02-01\" # \"2023-05-15\"\n",
        "azure_endpoint = \"https://eygpt24.openai.azure.com/\"\n",
        "model_name = \"gpt-4o\"\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
        "client = AzureOpenAI(\n",
        "    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
        "    api_version=api_version,\n",
        "    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
        "    azure_endpoint=azure_endpoint,\n",
        "    api_key = api_key,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "ksX0sguJZK0m"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lslYAvw37JGQ"
      },
      "outputs": [],
      "source": [
        "def generate_response(prompt,temp=0.0):\n",
        "  response = client.chat.completions.create(\n",
        "      messages=[{\"role\":\"system\",'content':\"you are helpful assistant, you answer questions in concise way\"},\n",
        "            {\"role\":\"user\",'content':prompt}],\n",
        "      model = model_name,\n",
        "      temperature=temp,\n",
        "  )\n",
        "  return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from Gemini\n",
        "predictions = [generate_response(input_text) for input_text in inputs]"
      ],
      "metadata": {
        "id": "AAtsTC5pzHji"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(references)):\n",
        "  print(f\"Reference {i+1}: {references[i][0]}\")\n",
        "  print(f\"Prediction {i+1}: {predictions[i]}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "Jmpe2kayIbW_",
        "outputId": "0d6b57d9-e4fd-4e9c-8778-b6322829fdf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1: Bonjour, comment ça va?\n",
            "Prediction 1: Bonjour, comment ça va ?\n",
            "\n",
            "Reference 2: The quick brown fox jumps over the lazy dog.\n",
            "Prediction 2: A quick brown fox jumps over a lazy dog.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity\n",
        "perplexities = [calculate_perplexity(calculate_word_probabilities(pred, ref[0])) for pred, ref in zip(predictions, references)]\n",
        "average_perplexity = sum(perplexities) / len(perplexities)\n",
        "print(f\"Average Perplexity: {average_perplexity}\")"
      ],
      "metadata": {
        "id": "tzkDA9FTz3TX",
        "outputId": "2568a48a-753a-4249-c40f-11f4d1872c54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Perplexity: 989.3459942200286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # BLEU Score Calculation\n",
        "bleu_scores = [calculate_bleu(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(f\"Average BLEU Score: {average_bleu}\")"
      ],
      "metadata": {
        "id": "F5_zOrk4IZwt",
        "outputId": "12aa75df-9d59-45d8-b712-5279a0c04371",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 0.2566725240200852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE Score\n",
        "rouge_scores = [calculate_rouge(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_rouge = {\n",
        "    'rouge1': sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rouge2': sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rougeL': sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "}\n",
        "print(f\"ROUGE 1 Score: {average_rouge['rouge1']}\")\n",
        "print(f\"ROUGE 2 Score: {average_rouge['rouge2']}\")\n",
        "print(f\"ROUGE L Score: {average_rouge['rougeL']}\")"
      ],
      "metadata": {
        "id": "15tm0vg9IutO",
        "outputId": "2826f911-06cd-4fec-8449-be89fdb197ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE 1 Score: 0.8888888888888888\n",
            "ROUGE 2 Score: 0.8125\n",
            "ROUGE L Score: 0.8888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_oRTiqyJNLy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "Here’s a quick summary of the metrics:\n",
        "\n",
        "| Metric      | Definition                                                                                       | Interpretation                                   | Benchmark                                 |\n",
        "|-------------|--------------------------------------------------------------------------------------------------|-------------------------------------------------|-------------------------------------------|\n",
        "| **Perplexity** | Measures how well a model predicts a sample. Lower values are better.                            | Low = Better (e.g., 10 or lower), High = Worse  | 10-50 for well-formed English text         |\n",
        "| **BLEU**      | Evaluates generated sentence against reference. Measures n-gram precision with penalty for short sentences. | High = Better (e.g., > 0.5), Low = Worse         | > 0.3 is reasonable, > 0.5 is good         |\n",
        "| **ROUGE**     | Measures n-gram overlap between generated and reference sentences. Focuses on recall.            | High = Better (e.g., > 0.5), Low = Worse         | > 0.5 for summarization tasks              |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bWeTI6RZUy0m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FC6S0HKGU0rv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}