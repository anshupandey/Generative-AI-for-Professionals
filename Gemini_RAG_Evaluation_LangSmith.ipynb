{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-for-Professionals/blob/main/Gemini_RAG_Evaluation_LangSmith.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e7db2b1-8f9c-46bd-9c50-b6cfb0a38a22",
      "metadata": {
        "id": "2e7db2b1-8f9c-46bd-9c50-b6cfb0a38a22"
      },
      "source": [
        "# RAG Evaluation\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/guides/evaluation/examples/rag.ipynb)\n",
        "\n",
        "RAG (Retrieval Augmented Generation) is one of the most popular techniques for building applications with LLMs.\n",
        "\n",
        "For an in-depth review, see our RAG series of notebooks and videos [here](https://github.com/langchain-ai/rag-from-scratch)).\n",
        "\n",
        "## Types of RAG eval\n",
        "\n",
        "There are at least 4 types of RAG eval that users of typically interested in (here, `<>` means \"compared against\"):\n",
        "\n",
        "1. **Response <> reference answer**: metrics like correctness measure \"*how similar/correct is the answer, relative to a ground-truth label*\"\n",
        "2. **Response <> input**: metrics like answer relevance, helpfulness, etc. measure \"*how well does the generated response address the initial user input*\"\n",
        "3. **Response <> retrieved docs**: metrics like faithfulness, hallucinations, etc. measure \"*to what extent does the generated response agree with the retrieved context*\"\n",
        "5. **Retrieved docs <> input**: metrics like score @ k, mean reciprocal rank, NDCG, etc. measure \"*how good are my retrieved results for this query*\"\n",
        "\n",
        "![](https://github.com/langchain-ai/langsmith-cookbook/blob/1a46c089ede410384f69dc3e808567406c39e009/testing-examples/rag_eval/langsmith_rag_eval.png?raw=1)\n",
        "\n",
        "\n",
        "Each of these evals has something in common: it will **compare** text against some grounding (e.g., answer vs reference answer, etc).\n",
        "\n",
        "We can use various built-in `LangChainStringEvaluator` types for this, but the same principles apply no matter which type of evaluator you are using. (see [here](https://docs.smith.langchain.com/evaluation/faq/evaluator-implementations#overview)).\n",
        "\n",
        "All `LangChainStringEvaluator` implementations can accept 3 inputs:\n",
        "\n",
        "```\n",
        "prediction: The prediction string.\n",
        "reference: The reference string.\n",
        "input: The input string.\n",
        "```\n",
        "\n",
        "`prediction` is always required\n",
        "\n",
        "`input` is required for most evaluators (`criteria`, `score_string`, `labeled_criteria`, `labeled_score_string`, `qa`, `cot_qa`)\n",
        "\n",
        "`reference` is required for labeled evaluators, which are evaluators that grade against an expected value (`qa`, `cot_qa`, `labeled_criteria`, `labeled_score_string`)\n",
        "\n",
        "\n",
        "Below, we will use this to perform eval.\n",
        "\n",
        "## RAG pipeline\n",
        "\n",
        "To start, we build a RAG pipeline. We will be using LangChain strictly for creating the retriever and retrieving the relevant documents. The overall pipeline does not use LangChain. LangSmith works regardless of whether or not your pipeline is built with LangChain.\n",
        "\n",
        "**Note** in the below example, we return the retrieved documents as part of the final answer. In a follow-up tutorial, we will showcase how to make use of these RAG evaluation techniques *even when your pipline returns only the final answer!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d809e9a0-44bc-4e9f-8eee-732ef077538c",
      "metadata": {
        "id": "d809e9a0-44bc-4e9f-8eee-732ef077538c",
        "outputId": "2674233a-984f-4c8c-ae8a-39d156f38fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m280.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langsmith langchain-community langchain tiktoken langchain-chroma --quiet\n",
        "!pip install langchain-google-vertexai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "SRJuR9nmfCRr",
        "outputId": "ec9588fa-09af-40df-9c01-7a715710e5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SRJuR9nmfCRr",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/295.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "760cab79-2d5e-4324-ba4a-54b6f4094cb0",
      "metadata": {
        "id": "760cab79-2d5e-4324-ba4a-54b6f4094cb0"
      },
      "source": [
        "We build an `index` using a set of LangChain docs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7UyNVSiyQ96"
      },
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, it is recommended to restart the runtime. Run the following cell to restart the current kernel.\n",
        "\n",
        "The restart process might take a minute or so.\n"
      ],
      "id": "j7UyNVSiyQ96"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmY9HVVGSBW5",
        "outputId": "749d2f1e-2295-4827-8073-9d13b8293dfc",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "id": "YmY9HVVGSBW5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TCvupe_UQvS"
      },
      "source": [
        "After the restart is complete, continue to the next step.\n"
      ],
      "id": "4TCvupe_UQvS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQZrM5hQeKb"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ Wait for the kernel to finish restarting before you continue. ⚠️</b>\n",
        "</div>\n"
      ],
      "id": "EXQZrM5hQeKb"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "33nVgznUPRGt"
      },
      "execution_count": 1,
      "outputs": [],
      "id": "33nVgznUPRGt"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fOy5DvJmOCbJ"
      },
      "outputs": [],
      "source": [
        "# TODO(developer): Update the below lines\n",
        "PROJECT_ID = \"jrproject-402905\"\n",
        "LOCATION = \"us-central1\"\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "id": "fOy5DvJmOCbJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "5s3YfpxmUSR-"
      },
      "execution_count": 3,
      "outputs": [],
      "id": "5s3YfpxmUSR-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXHfaVS66_01",
        "tags": []
      },
      "source": [
        "## Import libraries\n"
      ],
      "id": "jXHfaVS66_01"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6f7c0017-f4dd-4071-aa48-40957ffb4e9d",
      "metadata": {
        "id": "6f7c0017-f4dd-4071-aa48-40957ffb4e9d"
      },
      "outputs": [],
      "source": [
        "### INDEX\n",
        "\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_paths = [\"https://www.morningstar.com/content/dam/marketing/shared/research/methodology/771945_Morningstar_Rating_for_Funds_Methodology.pdf\",\n",
        "             \"https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf\"]\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "loaders = [PyPDFLoader(pdf, extract_images=False) for pdf in doc_paths]\n",
        "\n",
        "docs = []\n",
        "\n",
        "for loader in loaders:\n",
        "    doc = loader.load()\n",
        "    docs.extend(doc)\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "Eevn3TRde8eO",
        "outputId": "4aa65018-6c10-4728-94b3-4eb03dad78ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Eevn3TRde8eO",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3500, chunk_overlap=500)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(len(splits))\n"
      ],
      "metadata": {
        "id": "khrtpCmSerLi",
        "outputId": "c68d3c88-9969-4281-fdb1-f721f409d042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "khrtpCmSerLi",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai.embeddings import VertexAIEmbeddings\n",
        "embedding_fun = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")"
      ],
      "metadata": {
        "id": "rb08m5VIfaC0"
      },
      "id": "rb08m5VIfaC0",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_fun)\n",
        "\n",
        "# Index\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "c1hBCDGJfNdn",
        "outputId": "393484c8-1281-4976-8863-99b1072c6e3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "c1hBCDGJfNdn",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n",
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n",
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = retriever.invoke(\"What is a style profile\")\n",
        "len(retrieved_docs)"
      ],
      "metadata": {
        "id": "lPXRqqutmSkV",
        "outputId": "93ac0cbb-d89d-48fd-aa31-3669288db113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lPXRqqutmSkV",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[1].page_content)"
      ],
      "metadata": {
        "id": "wnB0RMhqmZDl",
        "outputId": "fffb320d-fe62-42ac-b614-7b2575823bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wnB0RMhqmZDl",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "©2021 Morningstar, Inc. All rights reserved. The information in this document is the property of Morningstar, Inc. Reproduction or transcription by any means, in whole or in part, without the prior written \n",
            "consent of Morningstar, Inc., is prohibited.\n",
            " The Morningstar RatingTM for Funds    August 2021 Page 3 of 21\n",
            "3\n",
            "3\n",
            "3bond funds domiciled in Europe against other European high-yield bond funds. For more information \n",
            "about available categories, please contact your local Morningstar office.\n",
            "Style Profiles\n",
            "A style profile may be considered a summary of a fund’s risk-factor exposures. Fund categories \n",
            "define groups of funds whose members are similar enough in their risk-factor exposures that return \n",
            "comparisons between them are useful.\n",
            "The risk factors on which fund categories are based can relate to value-growth orientation; \n",
            "capitalization; industry sector, geographic region, and country weights; duration and credit quality; \n",
            "historical return volatility; beta; and many other investment style factors. The specific factors used \n",
            "are considered to be a) important in explaining fund-return differences and b) actively controlled by \n",
            "the fund managers.\n",
            " \n",
            "Because the funds in a given category are similar in their risk-factor exposures, the observed return \n",
            "differences among them relate primarily to security selection (“stock-picking”) or to variation in \n",
            "the timing and amount of exposure to the risk factors that collectively define the category (“asset \n",
            "weighting”). Each of these, over time, may be presumed to have been a skill-related effect.\n",
            "Note that if all members of a fund category were uniform and consistent in their risk factor \n",
            "exposures, and the risk factors were comprehensive, there would be no need to risk-adjust returns \n",
            "when creating category-based star ratings. However, even within a tightly defined category,  \n",
            "the risk exposures of individual funds vary over time. Also, no style profile or category definition is \n",
            "comprehensive enough to capture all risk factors that affect the returns of the funds within  \n",
            "a category.\n",
            "In extreme cases where the funds in a category vary widely in their risk factor exposures (that is,  \n",
            "it is a “convenience category”), a star rating would have little value and is not assigned. For example, \n",
            "in the United States, ratings are not assigned to funds in the bear-market category because  \n",
            "these funds short very different parts of the market. In Europe, ratings are not assigned to funds in \n",
            "the guaranteed category.\n",
            "Defining Fund Categories\n",
            "The following considerations apply when Morningstar defines fund categories:\n",
            "Funds are grouped by the types of investment exposures that dominate their portfolios. \n",
            "In general, a single return benchmark should form a valid basis for evaluating the returns for all funds \n",
            "in a single category (that is, for performance attribution).\n",
            "In general, funds in the same category can be considered reasonable substitutes for the purposes of \n",
            "portfolio construction.\n",
            "Category membership is based on a fund’s long-term or “normal” style profile, based on three \n",
            "years of portfolio statistics. Supplemental analysis includes returns-based style analysis, review of \n",
            "strategy disclosure from fund literature, and qualitative review by analysts.  3\n",
            "3\n",
            "3\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c365fb82-78a6-40b6-bd59-daaa1e79d6c8",
      "metadata": {
        "id": "c365fb82-78a6-40b6-bd59-daaa1e79d6c8"
      },
      "source": [
        "Next, we build a `RAG chain` that returns an `answer` and the retrieved documents as `contexts`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "llm = ChatVertexAI(model=\"gemini-1.5-flash-001\")\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "]\n",
        "\n",
        "llm.invoke(messages).content\n"
      ],
      "metadata": {
        "id": "8LUwfdqmiHF1",
        "outputId": "2372f63b-75b4-4d38-b307-efcc1581d7cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "8LUwfdqmiHF1",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ciao! \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "68e249d7-bc6c-4631-b099-6daaeeddf38a",
      "metadata": {
        "id": "68e249d7-bc6c-4631-b099-6daaeeddf38a"
      },
      "outputs": [],
      "source": [
        "### RAG\n",
        "\n",
        "\n",
        "from langsmith import traceable\n",
        "from langsmith.wrappers import wrap_openai\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "message = \"\"\"\n",
        "Answer this question using the provided context only.\n",
        "\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
        "\n",
        "class RagBot:\n",
        "\n",
        "    def __init__(self, retriever, model):\n",
        "        self._retriever = retriever\n",
        "        # Wrapping the client instruments the LLM\n",
        "        self.model = llm\n",
        "\n",
        "    @traceable()\n",
        "    def retrieve_docs(self, question):\n",
        "        return self._retriever.invoke(question)\n",
        "\n",
        "    @traceable()\n",
        "    def get_answer(self, question: str):\n",
        "        context = self.retrieve_docs(question)\n",
        "        print(context)\n",
        "        self.chain = prompt | self.model\n",
        "        response = self.chain.invoke({\"context\":context,\"question\":question})\n",
        "\n",
        "        # Evaluators will expect \"answer\" and \"contexts\"\n",
        "        return {\n",
        "            \"answer\": response.content,\n",
        "            \"contexts\": [str(doc) for doc in context],\n",
        "        }\n",
        "\n",
        "\n",
        "rag_bot = RagBot(retriever,model=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6101d155-a1ab-460c-8c3e-f1f44e09a8b7",
      "metadata": {
        "id": "6101d155-a1ab-460c-8c3e-f1f44e09a8b7",
        "outputId": "eb80f71f-7221-4325-f240-95754b2abee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.'), Document(metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Investors can use Morningstar Sustainability Ratings to:\\n\\n* **Assess ESG risk and opportunity management:**  The ratings help investors understand how'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "response = rag_bot.get_answer(\"How should investors use our Sustainability Ratings?\") # what is Portfolio ESG Score?\n",
        "response[\"answer\"][:150]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['answer']"
      ],
      "metadata": {
        "id": "hZ8x393jqIcz",
        "outputId": "ae642904-2060-4609-94c5-95f4f4629d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "id": "hZ8x393jqIcz",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Portfolio ESG Score is an asset-weighted average of normalized company-level ESG Scores for the covered holdings in the portfolio. These company-level ESG Scores come from Sustainalytics and reflect companies' management systems, practices, policies, and other indicators related to environmental, social, and governance issues. They are scored on a scale of 0-100, with a higher score indicating better performance. A high Portfolio ESG Score means that a fund has more of its assets invested in companies that score well according to the Sustainalytics ESG methodology. \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "W6f-xRR17yLJ",
        "outputId": "2ff25e36-a736-4ebc-99c6-df5eca67daf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "W6f-xRR17yLJ",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': \"Investors can use Morningstar Sustainability Ratings to:\\n\\n* **Assess ESG risk and opportunity management:**  The ratings help investors understand how well the companies within a fund's portfolio are managing their environmental, social, and governance (ESG) risks and opportunities.\\n* **Compare funds:** The ratings allow investors to compare funds across different categories and relative to benchmarks based on specific ESG factors.\\n* **Screen for sustainability:** The ratings can serve as an initial screen for investors who are interested in sustainability and ESG factors.\\n* **Understand investment process:** The ratings can provide insights into a fund manager's investment process and how it relates to\",\n",
              " 'contexts': [\"page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.' metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}\",\n",
              "  \"page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.' metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "432e8ec7-a085-4224-ad38-0087e1d553f1",
      "metadata": {
        "id": "432e8ec7-a085-4224-ad38-0087e1d553f1"
      },
      "source": [
        "## RAG Dataset\n",
        "\n",
        "Next, we build a dataset of QA pairs based upon the [documentation](https://python.langchain.com/docs/expression_language/) that we indexed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "f11ddd7e-5c51-479c-b110-455086020d16",
      "metadata": {
        "id": "f11ddd7e-5c51-479c-b110-455086020d16"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "0f29304f-d79b-40e9-988a-343732102af9",
      "metadata": {
        "id": "0f29304f-d79b-40e9-988a-343732102af9",
        "outputId": "c0965ae2-54fb-4ab0-b729-0c3644d2908e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LangSmithConflictError",
          "evalue": "Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langsmith/utils.py\u001b[0m in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langsmith/client.py\u001b[0m in \u001b[0;36mrequest_with_retries\u001b[0;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m                         )\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0mls_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status_with_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langsmith/utils.py\u001b[0m in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLangSmithConflictError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-dcbb9d30565f>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RAG_test_Finance\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m dataset = client.create_dataset(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"QA pairs about Finance.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langsmith/client.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, dataset_name, description, data_type)\u001b[0m\n\u001b[1;32m   2532\u001b[0m             \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m         )\n\u001b[0;32m-> 2534\u001b[0;31m         response = self.request_with_retries(\n\u001b[0m\u001b[1;32m   2535\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2536\u001b[0m             \u001b[0;34m\"/datasets\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langsmith/client.py\u001b[0m in \u001b[0;36mrequest_with_retries\u001b[0;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m                             )\n\u001b[1;32m    855\u001b[0m                         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                             raise ls_utils.LangSmithConflictError(\n\u001b[0m\u001b[1;32m    857\u001b[0m                                 \u001b[0;34mf\"Conflict for {pathname}. {repr(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                             )\n",
            "\u001b[0;31mLangSmithConflictError\u001b[0m: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')"
          ]
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "# QA\n",
        "inputs = [\n",
        "    \"What is a style profile?\",\n",
        "    \"How should investors use our Sustainability Ratings?\",\n",
        "    \"what is Portfolio ESG Score?\",\n",
        "]\n",
        "\n",
        "outputs = [\n",
        "    \"A style profile is a summary of a fund's risk-factor exposures. It outlines the factors that influence a fund's returns, such as value-growth orientation, capitalization, industry sector, geographic region, country weights, duration, credit quality, historical return volatility, beta, and other investment style factors.\",\n",
        "    \"Investors can use Morningstar Sustainability Ratings to assess how well companies in a fund manage ESG risks and opportunities, compare funds across categories, screen for sustainability, and understand a fund manager's investment process regarding ESG factors.\",\n",
        "    \"The Portfolio ESG Score, provided by Sustainalytics, is an asset-weighted average of normalized company-level ESG scores, ranging from 0-100. A higher score indicates better ESG performance, with funds investing more in companies that excel in managing environmental, social, and governance issues.\",\n",
        "]\n",
        "\n",
        "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
        "\n",
        "# Create dataset\n",
        "client = Client()\n",
        "dataset_name = \"RAG_test_Finance\"\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"QA pairs about Finance.\",\n",
        ")\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\": q} for q in inputs],\n",
        "    outputs=[{\"answer\": a} for a in outputs],\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92cf3a0f-621f-468d-818d-a6f2d4b53823",
      "metadata": {
        "id": "92cf3a0f-621f-468d-818d-a6f2d4b53823"
      },
      "source": [
        "## RAG Evaluators\n",
        "\n",
        "### Type 1: Reference Answer\n",
        "\n",
        "First, lets consider the case in which we want to compare our RAG chain answer to a reference answer.\n",
        "\n",
        "This is shown on the far right (blue):\n",
        "\n",
        "![](https://github.com/langchain-ai/langsmith-cookbook/blob/1a46c089ede410384f69dc3e808567406c39e009/testing-examples/rag_eval/langsmith_rag_eval.png?raw=1)\n",
        "\n",
        "Here is the eval process we will use:\n",
        "\n",
        "![](https://github.com/langchain-ai/langsmith-cookbook/blob/1a46c089ede410384f69dc3e808567406c39e009/testing-examples/rag_eval/langsmith_rag_story.png?raw=1)\n",
        "\n",
        "#### Eval flow\n",
        "\n",
        "We will use a `LangChainStringEvaluator` to compare RAG chain answers to reference (ground truth) answers.\n",
        "\n",
        "There are many types of `LangChainStringEvaluator` [see options](https://docs.smith.langchain.com/evaluation/faq/evaluator-implementations).\n",
        "\n",
        "For comparing questions and answers, I like to use `LLM-as-judge` evaluators:\n",
        "* `QA`\n",
        "* `CoTQA`\n",
        "\n",
        "For example, `CoT_QA` uses the eval prompt defined [here](https://smith.langchain.com/hub/langchain-ai/cot_qa).\n",
        "\n",
        "And all `LangChainStringEvaluator` expose a common interface to pass the chain and dataset inputs:\n",
        "\n",
        "1. `question` from the dataset -> `input`\n",
        "2. `answer` from the dataset -> `reference`\n",
        "3. `answer` from the LLM -> `prediction`\n",
        "\n",
        "![](https://github.com/langchain-ai/langsmith-cookbook/blob/1a46c089ede410384f69dc3e808567406c39e009/testing-examples/rag_eval/langsmith_rag_flow.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "1cbe0b4a-2a30-4f40-b3aa-5cc67c6a7802",
      "metadata": {
        "id": "1cbe0b4a-2a30-4f40-b3aa-5cc67c6a7802"
      },
      "outputs": [],
      "source": [
        "# RAG chain\n",
        "def predict_rag_answer(example: dict):\n",
        "    \"\"\"Use this for answer evaluation\"\"\"\n",
        "    response = rag_bot.get_answer(example[\"question\"])\n",
        "    return {\"answer\": response[\"answer\"]}\n",
        "\n",
        "def predict_rag_answer_with_context(example: dict):\n",
        "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
        "    response = rag_bot.get_answer(example[\"question\"])\n",
        "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "a7a3827d-a92f-4a7a-a572-5123fbd9c334",
      "metadata": {
        "id": "a7a3827d-a92f-4a7a-a572-5123fbd9c334",
        "outputId": "1e05a7ab-2fa0-432c-a3cd-3fb8d3430aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "3c0c0410bf8645beb7b83e703dc8c861",
            "123d11fd237f4c41ab6ca993edad0a13",
            "eca1f39306e94fc8a7ea11c673eedd86",
            "f77682e0767b4167b00966a674a77b07",
            "ce1ee936cc084ffb93afc5fc8b45e71d",
            "27e134399c7743cda47a54c268a50385",
            "13cf91d3075f4cac8b2d7ac2f359681a",
            "98d3d8612b4e45fc84e3e7e3258f5fb7",
            "5095e4d3ace243168737a4841d17e377",
            "74862c20eaf14739a72747bd98f2d844",
            "d2270e44610641988864b3767276acb5"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'rag-qa-oai-29165536' at:\n",
            "https://smith.langchain.com/o/e5ffb10d-d7d5-5a52-9eb0-1e8ca168f073/datasets/c077395e-3ec2-4f33-8e26-85cac805d423/compare?selectedSessions=a2eaa497-6e16-4ca3-9da2-12a41fb4adba\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c0c0410bf8645beb7b83e703dc8c861"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n",
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.'), Document(metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.')]\n",
            "[Document(metadata={'page': 2, 'source': 'https://www.morningstar.com/content/dam/marketing/shared/research/methodology/771945_Morningstar_Rating_for_Funds_Methodology.pdf'}, page_content='©2021 Morningstar, Inc. All rights reserved. The information in this document is the property of Morningstar, Inc. Reproduction or transcription by any means, in whole or in part, without the prior written \\nconsent of Morningstar, Inc., is prohibited.\\n The Morningstar RatingTM for Funds    August 2021 Page 3 of 21\\n3\\n3\\n3bond funds domiciled in Europe against other European high-yield bond funds. For more information \\nabout available categories, please contact your local Morningstar office.\\nStyle Profiles\\nA style profile may be considered a summary of a fund’s risk-factor exposures. Fund categories \\ndefine groups of funds whose members are similar enough in their risk-factor exposures that return \\ncomparisons between them are useful.\\nThe risk factors on which fund categories are based can relate to value-growth orientation; \\ncapitalization; industry sector, geographic region, and country weights; duration and credit quality; \\nhistorical return volatility; beta; and many other investment style factors. The specific factors used \\nare considered to be a) important in explaining fund-return differences and b) actively controlled by \\nthe fund managers.\\n \\nBecause the funds in a given category are similar in their risk-factor exposures, the observed return \\ndifferences among them relate primarily to security selection (“stock-picking”) or to variation in \\nthe timing and amount of exposure to the risk factors that collectively define the category (“asset \\nweighting”). Each of these, over time, may be presumed to have been a skill-related effect.\\nNote that if all members of a fund category were uniform and consistent in their risk factor \\nexposures, and the risk factors were comprehensive, there would be no need to risk-adjust returns \\nwhen creating category-based star ratings. However, even within a tightly defined category,  \\nthe risk exposures of individual funds vary over time. Also, no style profile or category definition is \\ncomprehensive enough to capture all risk factors that affect the returns of the funds within  \\na category.\\nIn extreme cases where the funds in a category vary widely in their risk factor exposures (that is,  \\nit is a “convenience category”), a star rating would have little value and is not assigned. For example, \\nin the United States, ratings are not assigned to funds in the bear-market category because  \\nthese funds short very different parts of the market. In Europe, ratings are not assigned to funds in \\nthe guaranteed category.\\nDefining Fund Categories\\nThe following considerations apply when Morningstar defines fund categories:\\nFunds are grouped by the types of investment exposures that dominate their portfolios. \\nIn general, a single return benchmark should form a valid basis for evaluating the returns for all funds \\nin a single category (that is, for performance attribution).\\nIn general, funds in the same category can be considered reasonable substitutes for the purposes of \\nportfolio construction.\\nCategory membership is based on a fund’s long-term or “normal” style profile, based on three \\nyears of portfolio statistics. Supplemental analysis includes returns-based style analysis, review of \\nstrategy disclosure from fund literature, and qualitative review by analysts.  3\\n3\\n3\\n3'), Document(metadata={'page': 2, 'source': 'https://www.morningstar.com/content/dam/marketing/shared/research/methodology/771945_Morningstar_Rating_for_Funds_Methodology.pdf'}, page_content='©2021 Morningstar, Inc. All rights reserved. The information in this document is the property of Morningstar, Inc. Reproduction or transcription by any means, in whole or in part, without the prior written \\nconsent of Morningstar, Inc., is prohibited.\\n The Morningstar RatingTM for Funds    August 2021 Page 3 of 21\\n3\\n3\\n3bond funds domiciled in Europe against other European high-yield bond funds. For more information \\nabout available categories, please contact your local Morningstar office.\\nStyle Profiles\\nA style profile may be considered a summary of a fund’s risk-factor exposures. Fund categories \\ndefine groups of funds whose members are similar enough in their risk-factor exposures that return \\ncomparisons between them are useful.\\nThe risk factors on which fund categories are based can relate to value-growth orientation; \\ncapitalization; industry sector, geographic region, and country weights; duration and credit quality; \\nhistorical return volatility; beta; and many other investment style factors. The specific factors used \\nare considered to be a) important in explaining fund-return differences and b) actively controlled by \\nthe fund managers.\\n \\nBecause the funds in a given category are similar in their risk-factor exposures, the observed return \\ndifferences among them relate primarily to security selection (“stock-picking”) or to variation in \\nthe timing and amount of exposure to the risk factors that collectively define the category (“asset \\nweighting”). Each of these, over time, may be presumed to have been a skill-related effect.\\nNote that if all members of a fund category were uniform and consistent in their risk factor \\nexposures, and the risk factors were comprehensive, there would be no need to risk-adjust returns \\nwhen creating category-based star ratings. However, even within a tightly defined category,  \\nthe risk exposures of individual funds vary over time. Also, no style profile or category definition is \\ncomprehensive enough to capture all risk factors that affect the returns of the funds within  \\na category.\\nIn extreme cases where the funds in a category vary widely in their risk factor exposures (that is,  \\nit is a “convenience category”), a star rating would have little value and is not assigned. For example, \\nin the United States, ratings are not assigned to funds in the bear-market category because  \\nthese funds short very different parts of the market. In Europe, ratings are not assigned to funds in \\nthe guaranteed category.\\nDefining Fund Categories\\nThe following considerations apply when Morningstar defines fund categories:\\nFunds are grouped by the types of investment exposures that dominate their portfolios. \\nIn general, a single return benchmark should form a valid basis for evaluating the returns for all funds \\nin a single category (that is, for performance attribution).\\nIn general, funds in the same category can be considered reasonable substitutes for the purposes of \\nportfolio construction.\\nCategory membership is based on a fund’s long-term or “normal” style profile, based on three \\nyears of portfolio statistics. Supplemental analysis includes returns-based style analysis, review of \\nstrategy disclosure from fund literature, and qualitative review by analysts.  3\\n3\\n3\\n3')]\n",
            "[Document(metadata={'page': 10, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook10Key Definitions\\n \\nPortfolio ESG Score\\nThe Portfolio ESG Score is an asset-weighted average of normalized company-level ESG  \\nScores for the covered holdings in the portfolio. Company-level ESG Scores from Sustainalytics \\nreflect companies’ management systems, practices, policies, and other indicators related to \\nenvironmental, social, and governance issues. Their company-level ESG scores use a 0-100  scale. \\nA high Portfolio ESG Score is better than a low score. At the portfolio level, high scores indicate \\nthat a fund has more of its assets invested in companies that score well according to the \\nSustainalytics ESG methodology.\\nPortfolio Controversy Score\\nThe Portfolio Controversy Score is the asset-weighted average level of the seriousness of the \\ncontroversial incidents related to companies in a fund’s portfolio. A low score is better than a high \\nscore, as it indicates the absence of controversies.\\nCompany-level controversial incident indicators from Sustainalytics reflect the most severe \\nESG-related controversy in which a firm is involved, classified as Low, Moderate, Significant,  \\nHigh, and Severe.\\nPortfolio Sustainability Score \\nThe Portfolio Sustainability Score is the Portfolio ESG Score minus the Portfolio  \\nControversy Score:\\nPortfolio Sustainability Score  =  Portfolio ESG Score  –  Portfolio Controversy Score/ 5\\nThe resulting score is displayed as a number between 0 and 100. From a sustainability  \\nstandpoint, a high score is better than a low score. A higher score indicates that a fund has,  \\non average, more of its assets invested in companies that score well according to the  \\nSustainalytics ESG methodology.'), Document(metadata={'page': 10, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook10Key Definitions\\n \\nPortfolio ESG Score\\nThe Portfolio ESG Score is an asset-weighted average of normalized company-level ESG  \\nScores for the covered holdings in the portfolio. Company-level ESG Scores from Sustainalytics \\nreflect companies’ management systems, practices, policies, and other indicators related to \\nenvironmental, social, and governance issues. Their company-level ESG scores use a 0-100  scale. \\nA high Portfolio ESG Score is better than a low score. At the portfolio level, high scores indicate \\nthat a fund has more of its assets invested in companies that score well according to the \\nSustainalytics ESG methodology.\\nPortfolio Controversy Score\\nThe Portfolio Controversy Score is the asset-weighted average level of the seriousness of the \\ncontroversial incidents related to companies in a fund’s portfolio. A low score is better than a high \\nscore, as it indicates the absence of controversies.\\nCompany-level controversial incident indicators from Sustainalytics reflect the most severe \\nESG-related controversy in which a firm is involved, classified as Low, Moderate, Significant,  \\nHigh, and Severe.\\nPortfolio Sustainability Score \\nThe Portfolio Sustainability Score is the Portfolio ESG Score minus the Portfolio  \\nControversy Score:\\nPortfolio Sustainability Score  =  Portfolio ESG Score  –  Portfolio Controversy Score/ 5\\nThe resulting score is displayed as a number between 0 and 100. From a sustainability  \\nstandpoint, a high score is better than a low score. A higher score indicates that a fund has,  \\non average, more of its assets invested in companies that score well according to the  \\nSustainalytics ESG methodology.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
          ]
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "# Evaluator\n",
        "qa_evalulator = [\n",
        "    LangChainStringEvaluator(\n",
        "        \"cot_qa\",\n",
        "        config={\"llm\": llm,},\n",
        "        prepare_data=lambda run, example: {\n",
        "            \"prediction\": run.outputs[\"answer\"],\n",
        "            \"reference\": example.outputs[\"answer\"],\n",
        "            \"input\": example.inputs[\"question\"],\n",
        "        },\n",
        "    )\n",
        "]\n",
        "dataset_name = \"RAG_test_Finance\"\n",
        "experiment_results = evaluate(\n",
        "    predict_rag_answer,\n",
        "    data=dataset_name,\n",
        "    evaluators=qa_evalulator,\n",
        "    experiment_prefix=\"rag-qa-oai\",\n",
        "    metadata={\"variant\": \"Finance context, Gemini Flash\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ba4123-c691-4aa0-ba76-e567e8aaf09f",
      "metadata": {
        "id": "60ba4123-c691-4aa0-ba76-e567e8aaf09f"
      },
      "source": [
        "### Type 2: Answer Hallucination\n",
        "\n",
        "Second, lets consider the case in which we want to compare our RAG chain answer to the retrieved documents.\n",
        "\n",
        "This is shown in the red in the top figure.\n",
        "\n",
        "#### Eval flow\n",
        "\n",
        "We will use a `LangChainStringEvaluator`, as mentioned above.\n",
        "\n",
        "There are many types of `LangChainStringEvaluator`.\n",
        "\n",
        "For comparing documents and answers, a common built-in `LangChainStringEvaluator` options is `Criteria` [here](https://python.langchain.com/docs/guides/productionization/evaluation/string/criteria_eval_chain/#using-reference-labels) because we want to supply custom criteria.\n",
        "\n",
        "We will use `labeled_score_string` as an LLM-as-judge evaluator, which uses the eval prompt defined [here](https://smith.langchain.com/hub/wfh/labeled-score-string).\n",
        "\n",
        "Here, we only need to use two inputs of the `LangChainStringEvaluator` interface:\n",
        "\n",
        "1. `contexts` from  LLM chain -> `reference`\n",
        "2. `answer` from the LLM chain -> `prediction`\n",
        "\n",
        "![](https://github.com/langchain-ai/langsmith-cookbook/blob/1a46c089ede410384f69dc3e808567406c39e009/testing-examples/rag_eval/langsmith_rag_flow_hallucination.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "7f0872a5-e989-415d-9fed-5846efaa9488",
      "metadata": {
        "id": "7f0872a5-e989-415d-9fed-5846efaa9488"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "answer_hallucination_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"\"\"Is the Assistant's Answer grounded in the Ground Truth documentation? A score of [[1]] means that the\n",
        "            Assistant answer contains is not at all based upon / grounded in the Groun Truth documentation. A score of [[5]] means\n",
        "            that the Assistant answer contains some information (e.g., a hallucination) that is not captured in the Ground Truth\n",
        "            documentation. A score of [[10]] means that the Assistant answer is fully based upon the in the Ground Truth documentation.\"\"\",\n",
        "        },\n",
        "         \"llm\": llm,\n",
        "        # If you want the score to be saved on a scale from 0 to 1\n",
        "        \"normalize_by\": 10,\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"answer\"],\n",
        "        \"reference\": run.outputs[\"contexts\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "6d5bf61b-3903-4cde-9ecf-67f0e0874521",
      "metadata": {
        "id": "6d5bf61b-3903-4cde-9ecf-67f0e0874521",
        "outputId": "9ed7072e-1831-4f05-9500-7373fa2d7ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954,
          "referenced_widgets": [
            "9e03451ccafd47b49dbb84af893331bb",
            "414240c21af14133a5dbc9426f99ce11",
            "1a233e2eaa6948e4b0f1b2539e5ea8a4",
            "5c43d666124b4d7eb2204e240bbb730a",
            "f5a893eec5b04076b44ab224c78b36d2",
            "5f4b98f7ddd948b0a15547eadf38796e",
            "d362a8385d7445569ee43390c6b84fda",
            "9a80f18b8e2140d3af555eec35db9018",
            "e60f177ac50e47c6a59e9e11d5fa0af3",
            "3469a9fe33d24624a354938a9cdb5b8a",
            "0904bb33ad2b4e11ae2db0130fe1417f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'rag-qa-oai-hallucination-82d1a8a0' at:\n",
            "https://smith.langchain.com/o/e5ffb10d-d7d5-5a52-9eb0-1e8ca168f073/datasets/c077395e-3ec2-4f33-8e26-85cac805d423/compare?selectedSessions=427749c9-7017-4d12-85fd-24da246c1369\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e03451ccafd47b49dbb84af893331bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n",
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'page': 10, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook10Key Definitions\\n \\nPortfolio ESG Score\\nThe Portfolio ESG Score is an asset-weighted average of normalized company-level ESG  \\nScores for the covered holdings in the portfolio. Company-level ESG Scores from Sustainalytics \\nreflect companies’ management systems, practices, policies, and other indicators related to \\nenvironmental, social, and governance issues. Their company-level ESG scores use a 0-100  scale. \\nA high Portfolio ESG Score is better than a low score. At the portfolio level, high scores indicate \\nthat a fund has more of its assets invested in companies that score well according to the \\nSustainalytics ESG methodology.\\nPortfolio Controversy Score\\nThe Portfolio Controversy Score is the asset-weighted average level of the seriousness of the \\ncontroversial incidents related to companies in a fund’s portfolio. A low score is better than a high \\nscore, as it indicates the absence of controversies.\\nCompany-level controversial incident indicators from Sustainalytics reflect the most severe \\nESG-related controversy in which a firm is involved, classified as Low, Moderate, Significant,  \\nHigh, and Severe.\\nPortfolio Sustainability Score \\nThe Portfolio Sustainability Score is the Portfolio ESG Score minus the Portfolio  \\nControversy Score:\\nPortfolio Sustainability Score  =  Portfolio ESG Score  –  Portfolio Controversy Score/ 5\\nThe resulting score is displayed as a number between 0 and 100. From a sustainability  \\nstandpoint, a high score is better than a low score. A higher score indicates that a fund has,  \\non average, more of its assets invested in companies that score well according to the  \\nSustainalytics ESG methodology.'), Document(metadata={'page': 10, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook10Key Definitions\\n \\nPortfolio ESG Score\\nThe Portfolio ESG Score is an asset-weighted average of normalized company-level ESG  \\nScores for the covered holdings in the portfolio. Company-level ESG Scores from Sustainalytics \\nreflect companies’ management systems, practices, policies, and other indicators related to \\nenvironmental, social, and governance issues. Their company-level ESG scores use a 0-100  scale. \\nA high Portfolio ESG Score is better than a low score. At the portfolio level, high scores indicate \\nthat a fund has more of its assets invested in companies that score well according to the \\nSustainalytics ESG methodology.\\nPortfolio Controversy Score\\nThe Portfolio Controversy Score is the asset-weighted average level of the seriousness of the \\ncontroversial incidents related to companies in a fund’s portfolio. A low score is better than a high \\nscore, as it indicates the absence of controversies.\\nCompany-level controversial incident indicators from Sustainalytics reflect the most severe \\nESG-related controversy in which a firm is involved, classified as Low, Moderate, Significant,  \\nHigh, and Severe.\\nPortfolio Sustainability Score \\nThe Portfolio Sustainability Score is the Portfolio ESG Score minus the Portfolio  \\nControversy Score:\\nPortfolio Sustainability Score  =  Portfolio ESG Score  –  Portfolio Controversy Score/ 5\\nThe resulting score is displayed as a number between 0 and 100. From a sustainability  \\nstandpoint, a high score is better than a low score. A higher score indicates that a fund has,  \\non average, more of its assets invested in companies that score well according to the  \\nSustainalytics ESG methodology.')]\n",
            "[Document(metadata={'page': 2, 'source': 'https://www.morningstar.com/content/dam/marketing/shared/research/methodology/771945_Morningstar_Rating_for_Funds_Methodology.pdf'}, page_content='©2021 Morningstar, Inc. All rights reserved. The information in this document is the property of Morningstar, Inc. Reproduction or transcription by any means, in whole or in part, without the prior written \\nconsent of Morningstar, Inc., is prohibited.\\n The Morningstar RatingTM for Funds    August 2021 Page 3 of 21\\n3\\n3\\n3bond funds domiciled in Europe against other European high-yield bond funds. For more information \\nabout available categories, please contact your local Morningstar office.\\nStyle Profiles\\nA style profile may be considered a summary of a fund’s risk-factor exposures. Fund categories \\ndefine groups of funds whose members are similar enough in their risk-factor exposures that return \\ncomparisons between them are useful.\\nThe risk factors on which fund categories are based can relate to value-growth orientation; \\ncapitalization; industry sector, geographic region, and country weights; duration and credit quality; \\nhistorical return volatility; beta; and many other investment style factors. The specific factors used \\nare considered to be a) important in explaining fund-return differences and b) actively controlled by \\nthe fund managers.\\n \\nBecause the funds in a given category are similar in their risk-factor exposures, the observed return \\ndifferences among them relate primarily to security selection (“stock-picking”) or to variation in \\nthe timing and amount of exposure to the risk factors that collectively define the category (“asset \\nweighting”). Each of these, over time, may be presumed to have been a skill-related effect.\\nNote that if all members of a fund category were uniform and consistent in their risk factor \\nexposures, and the risk factors were comprehensive, there would be no need to risk-adjust returns \\nwhen creating category-based star ratings. However, even within a tightly defined category,  \\nthe risk exposures of individual funds vary over time. Also, no style profile or category definition is \\ncomprehensive enough to capture all risk factors that affect the returns of the funds within  \\na category.\\nIn extreme cases where the funds in a category vary widely in their risk factor exposures (that is,  \\nit is a “convenience category”), a star rating would have little value and is not assigned. For example, \\nin the United States, ratings are not assigned to funds in the bear-market category because  \\nthese funds short very different parts of the market. In Europe, ratings are not assigned to funds in \\nthe guaranteed category.\\nDefining Fund Categories\\nThe following considerations apply when Morningstar defines fund categories:\\nFunds are grouped by the types of investment exposures that dominate their portfolios. \\nIn general, a single return benchmark should form a valid basis for evaluating the returns for all funds \\nin a single category (that is, for performance attribution).\\nIn general, funds in the same category can be considered reasonable substitutes for the purposes of \\nportfolio construction.\\nCategory membership is based on a fund’s long-term or “normal” style profile, based on three \\nyears of portfolio statistics. Supplemental analysis includes returns-based style analysis, review of \\nstrategy disclosure from fund literature, and qualitative review by analysts.  3\\n3\\n3\\n3'), Document(metadata={'page': 2, 'source': 'https://www.morningstar.com/content/dam/marketing/shared/research/methodology/771945_Morningstar_Rating_for_Funds_Methodology.pdf'}, page_content='©2021 Morningstar, Inc. All rights reserved. The information in this document is the property of Morningstar, Inc. Reproduction or transcription by any means, in whole or in part, without the prior written \\nconsent of Morningstar, Inc., is prohibited.\\n The Morningstar RatingTM for Funds    August 2021 Page 3 of 21\\n3\\n3\\n3bond funds domiciled in Europe against other European high-yield bond funds. For more information \\nabout available categories, please contact your local Morningstar office.\\nStyle Profiles\\nA style profile may be considered a summary of a fund’s risk-factor exposures. Fund categories \\ndefine groups of funds whose members are similar enough in their risk-factor exposures that return \\ncomparisons between them are useful.\\nThe risk factors on which fund categories are based can relate to value-growth orientation; \\ncapitalization; industry sector, geographic region, and country weights; duration and credit quality; \\nhistorical return volatility; beta; and many other investment style factors. The specific factors used \\nare considered to be a) important in explaining fund-return differences and b) actively controlled by \\nthe fund managers.\\n \\nBecause the funds in a given category are similar in their risk-factor exposures, the observed return \\ndifferences among them relate primarily to security selection (“stock-picking”) or to variation in \\nthe timing and amount of exposure to the risk factors that collectively define the category (“asset \\nweighting”). Each of these, over time, may be presumed to have been a skill-related effect.\\nNote that if all members of a fund category were uniform and consistent in their risk factor \\nexposures, and the risk factors were comprehensive, there would be no need to risk-adjust returns \\nwhen creating category-based star ratings. However, even within a tightly defined category,  \\nthe risk exposures of individual funds vary over time. Also, no style profile or category definition is \\ncomprehensive enough to capture all risk factors that affect the returns of the funds within  \\na category.\\nIn extreme cases where the funds in a category vary widely in their risk factor exposures (that is,  \\nit is a “convenience category”), a star rating would have little value and is not assigned. For example, \\nin the United States, ratings are not assigned to funds in the bear-market category because  \\nthese funds short very different parts of the market. In Europe, ratings are not assigned to funds in \\nthe guaranteed category.\\nDefining Fund Categories\\nThe following considerations apply when Morningstar defines fund categories:\\nFunds are grouped by the types of investment exposures that dominate their portfolios. \\nIn general, a single return benchmark should form a valid basis for evaluating the returns for all funds \\nin a single category (that is, for performance attribution).\\nIn general, funds in the same category can be considered reasonable substitutes for the purposes of \\nportfolio construction.\\nCategory membership is based on a fund’s long-term or “normal” style profile, based on three \\nyears of portfolio statistics. Supplemental analysis includes returns-based style analysis, review of \\nstrategy disclosure from fund literature, and qualitative review by analysts.  3\\n3\\n3\\n3')]\n",
            "[Document(metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.'), Document(metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "ERROR:langsmith.evaluation._runner:Error running target function: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
            "ERROR:langsmith.evaluation._runner:Error running target function: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 5a34a967-1501-4991-979d-55220030fe9b: KeyError('answer')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 1258, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 582, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 579, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "  File \"<ipython-input-52-500a8605fcdb>\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"answer\"],\n",
            "KeyError: 'answer'\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 3b1ecfcb-1ffc-4c35-9b19-eb9d94aaf284: KeyError('answer')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 1258, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 582, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 579, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "  File \"<ipython-input-52-500a8605fcdb>\", line 17, in <lambda>\n",
            "    \"prediction\": run.outputs[\"answer\"],\n",
            "KeyError: 'answer'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "experiment_results = evaluate(\n",
        "    predict_rag_answer_with_context,\n",
        "    data=dataset_name,\n",
        "    evaluators=[answer_hallucination_evaluator],\n",
        "    experiment_prefix=\"rag-qa-oai-hallucination\",\n",
        "    # Any experiment metadata can be specified here\n",
        "    metadata={\n",
        "        \"variant\": \"LCEL context, gpt-3.5-turbo\",\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "480a27cb-1a31-4194-b160-8cdcfbf24eea",
      "metadata": {
        "id": "480a27cb-1a31-4194-b160-8cdcfbf24eea"
      },
      "source": [
        "### Type 3: Document Relevance to Question\n",
        "\n",
        "Finally, lets consider the case in which we want to compare our RAG chain document retrieval to the question.\n",
        "\n",
        "This is shown in green in the top figure.\n",
        "\n",
        "#### Eval flow\n",
        "\n",
        "We will use a `LangChainStringEvaluator`, as mentioned above.\n",
        "\n",
        "For comparing documents and answers, common built-in `LangChainStringEvaluator` options are `Criteria` [here](https://python.langchain.com/docs/guides/productionization/evaluation/string/criteria_eval_chain/#using-reference-labels) because we want to supply custom criteria.\n",
        "\n",
        "We will use `score_string` as an LLM-as-judge evaluator [(docs)](https://docs.smith.langchain.com/evaluation/faq/evaluator-implementations#criteria-evaluators-no-labels), which uses the eval prompt defined [here](https://smith.langchain.com/hub/wfh/score-string).\n",
        "\n",
        "Here, we only need to use two inputs of the `LangChainStringEvaluator` interface:\n",
        "\n",
        "1. `question` from  LLM chain -> `reference`\n",
        "2. `contexts` from the LLM chain -> `prediction`\n",
        "\n",
        "![](https://github.com/langchain-ai/langsmith-cookbook/blob/1a46c089ede410384f69dc3e808567406c39e009/testing-examples/rag_eval/langsmith_rag_flow_doc_relevance.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "df247034-14ed-40b1-b313-b0fef7286546",
      "metadata": {
        "id": "df247034-14ed-40b1-b313-b0fef7286546"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "import textwrap\n",
        "\n",
        "docs_relevance_evaluator = LangChainStringEvaluator(\n",
        "    \"score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"document_relevance\": textwrap.dedent(\n",
        "                \"\"\"The response is a set of documents retrieved from a vectorstore. The input is a question\n",
        "            used for retrieval. You will score whether the Assistant's response (retrieved docs) is relevant to the Ground Truth\n",
        "            question. A score of [[1]] means that none of the  Assistant's response documents contain information useful in answering or addressing the user's input.\n",
        "            A score of [[5]] means that the Assistant answer contains some relevant documents that can at least partially answer the user's question or input.\n",
        "            A score of [[10]] means that the user input can be fully answered using the content in the first retrieved doc(s).\"\"\"\n",
        "            )\n",
        "        },\n",
        "         \"llm\": llm,\n",
        "        # If you want the score to be saved on a scale from 0 to 1\n",
        "        \"normalize_by\": 10,\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"contexts\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "cfe988dc-2aaa-42f4-93ff-c3c9fe6b3124",
      "metadata": {
        "id": "cfe988dc-2aaa-42f4-93ff-c3c9fe6b3124",
        "outputId": "92b43273-28b6-434a-8478-09a6c9c91393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1ef3f47300c5421d84e113bf7f0af907",
            "02bfb7f629f84acab0cf73f8880e8ff7",
            "29432ed5b90544f9b1627fc2f8568c09",
            "6cdf8546db524c349a7faa54f6addcf4",
            "6dbe654ec2b64118bcf8ca3f6fb7888e",
            "118d8c39c56f4a8990ad41fbc2311c50",
            "544266fa880546bd8f5481aa3d013b6d",
            "499889b7cc5e4188ac6e27ea3919cda1",
            "a56d3f9a6a5a4877a358ea06ad8873f9",
            "d2da99df97c140ba936e1bd42b7f9d1a",
            "df8b3d0dbccf4fd1b477c33c65c88c48"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'rag-qa-oai-doc-relevance-1ce90523' at:\n",
            "https://smith.langchain.com/o/e5ffb10d-d7d5-5a52-9eb0-1e8ca168f073/datasets/c077395e-3ec2-4f33-8e26-85cac805d423/compare?selectedSessions=227d3e2d-8172-41e0-83b1-bc53ac99fd52\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ef3f47300c5421d84e113bf7f0af907"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.'), Document(metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n",
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'page': 10, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook10Key Definitions\\n \\nPortfolio ESG Score\\nThe Portfolio ESG Score is an asset-weighted average of normalized company-level ESG  \\nScores for the covered holdings in the portfolio. Company-level ESG Scores from Sustainalytics \\nreflect companies’ management systems, practices, policies, and other indicators related to \\nenvironmental, social, and governance issues. Their company-level ESG scores use a 0-100  scale. \\nA high Portfolio ESG Score is better than a low score. At the portfolio level, high scores indicate \\nthat a fund has more of its assets invested in companies that score well according to the \\nSustainalytics ESG methodology.\\nPortfolio Controversy Score\\nThe Portfolio Controversy Score is the asset-weighted average level of the seriousness of the \\ncontroversial incidents related to companies in a fund’s portfolio. A low score is better than a high \\nscore, as it indicates the absence of controversies.\\nCompany-level controversial incident indicators from Sustainalytics reflect the most severe \\nESG-related controversy in which a firm is involved, classified as Low, Moderate, Significant,  \\nHigh, and Severe.\\nPortfolio Sustainability Score \\nThe Portfolio Sustainability Score is the Portfolio ESG Score minus the Portfolio  \\nControversy Score:\\nPortfolio Sustainability Score  =  Portfolio ESG Score  –  Portfolio Controversy Score/ 5\\nThe resulting score is displayed as a number between 0 and 100. From a sustainability  \\nstandpoint, a high score is better than a low score. A higher score indicates that a fund has,  \\non average, more of its assets invested in companies that score well according to the  \\nSustainalytics ESG methodology.'), Document(metadata={'page': 10, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook10Key Definitions\\n \\nPortfolio ESG Score\\nThe Portfolio ESG Score is an asset-weighted average of normalized company-level ESG  \\nScores for the covered holdings in the portfolio. Company-level ESG Scores from Sustainalytics \\nreflect companies’ management systems, practices, policies, and other indicators related to \\nenvironmental, social, and governance issues. Their company-level ESG scores use a 0-100  scale. \\nA high Portfolio ESG Score is better than a low score. At the portfolio level, high scores indicate \\nthat a fund has more of its assets invested in companies that score well according to the \\nSustainalytics ESG methodology.\\nPortfolio Controversy Score\\nThe Portfolio Controversy Score is the asset-weighted average level of the seriousness of the \\ncontroversial incidents related to companies in a fund’s portfolio. A low score is better than a high \\nscore, as it indicates the absence of controversies.\\nCompany-level controversial incident indicators from Sustainalytics reflect the most severe \\nESG-related controversy in which a firm is involved, classified as Low, Moderate, Significant,  \\nHigh, and Severe.\\nPortfolio Sustainability Score \\nThe Portfolio Sustainability Score is the Portfolio ESG Score minus the Portfolio  \\nControversy Score:\\nPortfolio Sustainability Score  =  Portfolio ESG Score  –  Portfolio Controversy Score/ 5\\nThe resulting score is displayed as a number between 0 and 100. From a sustainability  \\nstandpoint, a high score is better than a low score. A higher score indicates that a fund has,  \\non average, more of its assets invested in companies that score well according to the  \\nSustainalytics ESG methodology.')]\n",
            "[Document(metadata={'page': 2, 'source': 'https://www.morningstar.com/content/dam/marketing/shared/research/methodology/771945_Morningstar_Rating_for_Funds_Methodology.pdf'}, page_content='©2021 Morningstar, Inc. All rights reserved. The information in this document is the property of Morningstar, Inc. Reproduction or transcription by any means, in whole or in part, without the prior written \\nconsent of Morningstar, Inc., is prohibited.\\n The Morningstar RatingTM for Funds    August 2021 Page 3 of 21\\n3\\n3\\n3bond funds domiciled in Europe against other European high-yield bond funds. For more information \\nabout available categories, please contact your local Morningstar office.\\nStyle Profiles\\nA style profile may be considered a summary of a fund’s risk-factor exposures. Fund categories \\ndefine groups of funds whose members are similar enough in their risk-factor exposures that return \\ncomparisons between them are useful.\\nThe risk factors on which fund categories are based can relate to value-growth orientation; \\ncapitalization; industry sector, geographic region, and country weights; duration and credit quality; \\nhistorical return volatility; beta; and many other investment style factors. The specific factors used \\nare considered to be a) important in explaining fund-return differences and b) actively controlled by \\nthe fund managers.\\n \\nBecause the funds in a given category are similar in their risk-factor exposures, the observed return \\ndifferences among them relate primarily to security selection (“stock-picking”) or to variation in \\nthe timing and amount of exposure to the risk factors that collectively define the category (“asset \\nweighting”). Each of these, over time, may be presumed to have been a skill-related effect.\\nNote that if all members of a fund category were uniform and consistent in their risk factor \\nexposures, and the risk factors were comprehensive, there would be no need to risk-adjust returns \\nwhen creating category-based star ratings. However, even within a tightly defined category,  \\nthe risk exposures of individual funds vary over time. Also, no style profile or category definition is \\ncomprehensive enough to capture all risk factors that affect the returns of the funds within  \\na category.\\nIn extreme cases where the funds in a category vary widely in their risk factor exposures (that is,  \\nit is a “convenience category”), a star rating would have little value and is not assigned. For example, \\nin the United States, ratings are not assigned to funds in the bear-market category because  \\nthese funds short very different parts of the market. In Europe, ratings are not assigned to funds in \\nthe guaranteed category.\\nDefining Fund Categories\\nThe following considerations apply when Morningstar defines fund categories:\\nFunds are grouped by the types of investment exposures that dominate their portfolios. \\nIn general, a single return benchmark should form a valid basis for evaluating the returns for all funds \\nin a single category (that is, for performance attribution).\\nIn general, funds in the same category can be considered reasonable substitutes for the purposes of \\nportfolio construction.\\nCategory membership is based on a fund’s long-term or “normal” style profile, based on three \\nyears of portfolio statistics. Supplemental analysis includes returns-based style analysis, review of \\nstrategy disclosure from fund literature, and qualitative review by analysts.  3\\n3\\n3\\n3'), Document(metadata={'page': 2, 'source': 'https://www.morningstar.com/content/dam/marketing/shared/research/methodology/771945_Morningstar_Rating_for_Funds_Methodology.pdf'}, page_content='©2021 Morningstar, Inc. All rights reserved. The information in this document is the property of Morningstar, Inc. Reproduction or transcription by any means, in whole or in part, without the prior written \\nconsent of Morningstar, Inc., is prohibited.\\n The Morningstar RatingTM for Funds    August 2021 Page 3 of 21\\n3\\n3\\n3bond funds domiciled in Europe against other European high-yield bond funds. For more information \\nabout available categories, please contact your local Morningstar office.\\nStyle Profiles\\nA style profile may be considered a summary of a fund’s risk-factor exposures. Fund categories \\ndefine groups of funds whose members are similar enough in their risk-factor exposures that return \\ncomparisons between them are useful.\\nThe risk factors on which fund categories are based can relate to value-growth orientation; \\ncapitalization; industry sector, geographic region, and country weights; duration and credit quality; \\nhistorical return volatility; beta; and many other investment style factors. The specific factors used \\nare considered to be a) important in explaining fund-return differences and b) actively controlled by \\nthe fund managers.\\n \\nBecause the funds in a given category are similar in their risk-factor exposures, the observed return \\ndifferences among them relate primarily to security selection (“stock-picking”) or to variation in \\nthe timing and amount of exposure to the risk factors that collectively define the category (“asset \\nweighting”). Each of these, over time, may be presumed to have been a skill-related effect.\\nNote that if all members of a fund category were uniform and consistent in their risk factor \\nexposures, and the risk factors were comprehensive, there would be no need to risk-adjust returns \\nwhen creating category-based star ratings. However, even within a tightly defined category,  \\nthe risk exposures of individual funds vary over time. Also, no style profile or category definition is \\ncomprehensive enough to capture all risk factors that affect the returns of the funds within  \\na category.\\nIn extreme cases where the funds in a category vary widely in their risk factor exposures (that is,  \\nit is a “convenience category”), a star rating would have little value and is not assigned. For example, \\nin the United States, ratings are not assigned to funds in the bear-market category because  \\nthese funds short very different parts of the market. In Europe, ratings are not assigned to funds in \\nthe guaranteed category.\\nDefining Fund Categories\\nThe following considerations apply when Morningstar defines fund categories:\\nFunds are grouped by the types of investment exposures that dominate their portfolios. \\nIn general, a single return benchmark should form a valid basis for evaluating the returns for all funds \\nin a single category (that is, for performance attribution).\\nIn general, funds in the same category can be considered reasonable substitutes for the purposes of \\nportfolio construction.\\nCategory membership is based on a fund’s long-term or “normal” style profile, based on three \\nyears of portfolio statistics. Supplemental analysis includes returns-based style analysis, review of \\nstrategy disclosure from fund literature, and qualitative review by analysts.  3\\n3\\n3\\n3')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "ERROR:langsmith.evaluation._runner:Error running target function: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 024e9e4f-24da-4fd2-8ce6-f2519f57f7d6: KeyError('contexts')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 1258, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 582, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 579, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/integrations/_langchain.py\", line 258, in evaluate\n",
            "    else self._prepare_data(run, example)\n",
            "  File \"<ipython-input-54-e131f9ce0f64>\", line 21, in <lambda>\n",
            "    \"prediction\": run.outputs[\"contexts\"],\n",
            "KeyError: 'contexts'\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 99839738-491d-4eeb-ad41-5f9aa1447e51: ResourceExhausted('Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\", line 1181, in __call__\n",
            "    return _end_unary_response_blocking(state, call, False, None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\", line 1006, in _end_unary_response_blocking\n",
            "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
            "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
            "\tstatus = StatusCode.RESOURCE_EXHAUSTED\n",
            "\tdetails = \"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\"\n",
            "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.251.111.95:443 {created_time:\"2024-07-25T03:26:51.419661621+00:00\", grpc_status:8, grpc_message:\"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\"}\"\n",
            ">\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 1258, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 582, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 579, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/evaluation/scoring/eval_chain.py\", line 353, in _evaluate_strings\n",
            "    result = self(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\", line 128, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\", line 140, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 703, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 560, in generate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 550, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 775, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 1134, in _generate\n",
            "    return self._generate_gemini(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 1279, in _generate_gemini\n",
            "    response = _completion_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 587, in _completion_with_retry\n",
            "    return _completion_with_retry_inner(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 336, in wrapped_f\n",
            "    return copy(f, *args, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 475, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 478, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 580, in _completion_with_retry_inner\n",
            "    return generation_method(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/client.py\", line 2287, in generate_content\n",
            "    response = rpc(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n",
            "    raise exceptions.from_grpc_error(exc) from exc\n",
            "google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run a65cb906-d846-42d3-ad9f-67764d5bdfb0: ResourceExhausted('Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\", line 1181, in __call__\n",
            "    return _end_unary_response_blocking(state, call, False, None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\", line 1006, in _end_unary_response_blocking\n",
            "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
            "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
            "\tstatus = StatusCode.RESOURCE_EXHAUSTED\n",
            "\tdetails = \"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\"\n",
            "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.251.111.95:443 {grpc_message:\"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\", grpc_status:8, created_time:\"2024-07-25T03:26:51.533733562+00:00\"}\"\n",
            ">\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 1258, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 582, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 579, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/evaluation/scoring/eval_chain.py\", line 353, in _evaluate_strings\n",
            "    result = self(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 383, in __call__\n",
            "    return self.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 166, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 156, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\", line 128, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\", line 140, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 703, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 560, in generate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 550, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 775, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 1134, in _generate\n",
            "    return self._generate_gemini(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 1279, in _generate_gemini\n",
            "    response = _completion_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 587, in _completion_with_retry\n",
            "    return _completion_with_retry_inner(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 336, in wrapped_f\n",
            "    return copy(f, *args, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 475, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 478, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 580, in _completion_with_retry_inner\n",
            "    return generation_method(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/client.py\", line 2287, in generate_content\n",
            "    response = rpc(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n",
            "    raise exceptions.from_grpc_error(exc) from exc\n",
            "google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n"
          ]
        }
      ],
      "source": [
        "experiment_results = evaluate(\n",
        "    predict_rag_answer_with_context,\n",
        "    data=dataset_name,\n",
        "    evaluators=[docs_relevance_evaluator],\n",
        "    experiment_prefix=\"rag-qa-oai-doc-relevance\",\n",
        "    # Any experiment metadata can be specified here\n",
        "    metadata={\n",
        "        \"variant\": \"LCEL context, gpt-3.5-turbo\",\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40cca574-1881-4168-8c63-2443290d89f6",
      "metadata": {
        "id": "40cca574-1881-4168-8c63-2443290d89f6"
      },
      "source": [
        "## Evaluating intermediate traces\n",
        "\n",
        "What if we didn't explicity return documents from our RAG chain?\n",
        "\n",
        "In this case, we can isolate them as intermediate chain values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "7baeca7c-b398-481d-9eb1-ce3ea73f3d8d",
      "metadata": {
        "id": "7baeca7c-b398-481d-9eb1-ce3ea73f3d8d",
        "outputId": "09b53a07-8748-483f-818f-06de57f61137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "66c7bcc923724d61b53616aebca12e78",
            "e72b54fdca494e5eac21c3b00699a55c",
            "0a129fe982da4cba9ca188336ef1e5df",
            "ca6c2501bc9049f69c136153dc7e932f",
            "ea7ad3cfa498490d8a4179edd0da8abd",
            "2a2ef2d99be44562b7daafdde085e07a",
            "8acb1f2ec4354deca2a277369a57a957",
            "b4c889b70ef644648b4c66fb0b5c8512",
            "4b0e854a4b68419ba7bb3868bb67756a",
            "633268d9bc164c059f335cbf78ea3193",
            "cf20a0b5033f443e8916eac14115dddc"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'LCEL context, gpt-3.5-turbo-f0f1db92' at:\n",
            "https://smith.langchain.com/o/e5ffb10d-d7d5-5a52-9eb0-1e8ca168f073/datasets/c077395e-3ec2-4f33-8e26-85cac805d423/compare?selectedSessions=5d6fe835-2ca6-4f72-a3d3-8d6102dd425b\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66c7bcc923724d61b53616aebca12e78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n",
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'page': 2, 'source': 'https://www.morningstar.com/content/dam/marketing/shared/research/methodology/771945_Morningstar_Rating_for_Funds_Methodology.pdf'}, page_content='©2021 Morningstar, Inc. All rights reserved. The information in this document is the property of Morningstar, Inc. Reproduction or transcription by any means, in whole or in part, without the prior written \\nconsent of Morningstar, Inc., is prohibited.\\n The Morningstar RatingTM for Funds    August 2021 Page 3 of 21\\n3\\n3\\n3bond funds domiciled in Europe against other European high-yield bond funds. For more information \\nabout available categories, please contact your local Morningstar office.\\nStyle Profiles\\nA style profile may be considered a summary of a fund’s risk-factor exposures. Fund categories \\ndefine groups of funds whose members are similar enough in their risk-factor exposures that return \\ncomparisons between them are useful.\\nThe risk factors on which fund categories are based can relate to value-growth orientation; \\ncapitalization; industry sector, geographic region, and country weights; duration and credit quality; \\nhistorical return volatility; beta; and many other investment style factors. The specific factors used \\nare considered to be a) important in explaining fund-return differences and b) actively controlled by \\nthe fund managers.\\n \\nBecause the funds in a given category are similar in their risk-factor exposures, the observed return \\ndifferences among them relate primarily to security selection (“stock-picking”) or to variation in \\nthe timing and amount of exposure to the risk factors that collectively define the category (“asset \\nweighting”). Each of these, over time, may be presumed to have been a skill-related effect.\\nNote that if all members of a fund category were uniform and consistent in their risk factor \\nexposures, and the risk factors were comprehensive, there would be no need to risk-adjust returns \\nwhen creating category-based star ratings. However, even within a tightly defined category,  \\nthe risk exposures of individual funds vary over time. Also, no style profile or category definition is \\ncomprehensive enough to capture all risk factors that affect the returns of the funds within  \\na category.\\nIn extreme cases where the funds in a category vary widely in their risk factor exposures (that is,  \\nit is a “convenience category”), a star rating would have little value and is not assigned. For example, \\nin the United States, ratings are not assigned to funds in the bear-market category because  \\nthese funds short very different parts of the market. In Europe, ratings are not assigned to funds in \\nthe guaranteed category.\\nDefining Fund Categories\\nThe following considerations apply when Morningstar defines fund categories:\\nFunds are grouped by the types of investment exposures that dominate their portfolios. \\nIn general, a single return benchmark should form a valid basis for evaluating the returns for all funds \\nin a single category (that is, for performance attribution).\\nIn general, funds in the same category can be considered reasonable substitutes for the purposes of \\nportfolio construction.\\nCategory membership is based on a fund’s long-term or “normal” style profile, based on three \\nyears of portfolio statistics. Supplemental analysis includes returns-based style analysis, review of \\nstrategy disclosure from fund literature, and qualitative review by analysts.  3\\n3\\n3\\n3'), Document(metadata={'page': 2, 'source': 'https://www.morningstar.com/content/dam/marketing/shared/research/methodology/771945_Morningstar_Rating_for_Funds_Methodology.pdf'}, page_content='©2021 Morningstar, Inc. All rights reserved. The information in this document is the property of Morningstar, Inc. Reproduction or transcription by any means, in whole or in part, without the prior written \\nconsent of Morningstar, Inc., is prohibited.\\n The Morningstar RatingTM for Funds    August 2021 Page 3 of 21\\n3\\n3\\n3bond funds domiciled in Europe against other European high-yield bond funds. For more information \\nabout available categories, please contact your local Morningstar office.\\nStyle Profiles\\nA style profile may be considered a summary of a fund’s risk-factor exposures. Fund categories \\ndefine groups of funds whose members are similar enough in their risk-factor exposures that return \\ncomparisons between them are useful.\\nThe risk factors on which fund categories are based can relate to value-growth orientation; \\ncapitalization; industry sector, geographic region, and country weights; duration and credit quality; \\nhistorical return volatility; beta; and many other investment style factors. The specific factors used \\nare considered to be a) important in explaining fund-return differences and b) actively controlled by \\nthe fund managers.\\n \\nBecause the funds in a given category are similar in their risk-factor exposures, the observed return \\ndifferences among them relate primarily to security selection (“stock-picking”) or to variation in \\nthe timing and amount of exposure to the risk factors that collectively define the category (“asset \\nweighting”). Each of these, over time, may be presumed to have been a skill-related effect.\\nNote that if all members of a fund category were uniform and consistent in their risk factor \\nexposures, and the risk factors were comprehensive, there would be no need to risk-adjust returns \\nwhen creating category-based star ratings. However, even within a tightly defined category,  \\nthe risk exposures of individual funds vary over time. Also, no style profile or category definition is \\ncomprehensive enough to capture all risk factors that affect the returns of the funds within  \\na category.\\nIn extreme cases where the funds in a category vary widely in their risk factor exposures (that is,  \\nit is a “convenience category”), a star rating would have little value and is not assigned. For example, \\nin the United States, ratings are not assigned to funds in the bear-market category because  \\nthese funds short very different parts of the market. In Europe, ratings are not assigned to funds in \\nthe guaranteed category.\\nDefining Fund Categories\\nThe following considerations apply when Morningstar defines fund categories:\\nFunds are grouped by the types of investment exposures that dominate their portfolios. \\nIn general, a single return benchmark should form a valid basis for evaluating the returns for all funds \\nin a single category (that is, for performance attribution).\\nIn general, funds in the same category can be considered reasonable substitutes for the purposes of \\nportfolio construction.\\nCategory membership is based on a fund’s long-term or “normal” style profile, based on three \\nyears of portfolio statistics. Supplemental analysis includes returns-based style analysis, review of \\nstrategy disclosure from fund literature, and qualitative review by analysts.  3\\n3\\n3\\n3')]\n",
            "[Document(metadata={'page': 10, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook10Key Definitions\\n \\nPortfolio ESG Score\\nThe Portfolio ESG Score is an asset-weighted average of normalized company-level ESG  \\nScores for the covered holdings in the portfolio. Company-level ESG Scores from Sustainalytics \\nreflect companies’ management systems, practices, policies, and other indicators related to \\nenvironmental, social, and governance issues. Their company-level ESG scores use a 0-100  scale. \\nA high Portfolio ESG Score is better than a low score. At the portfolio level, high scores indicate \\nthat a fund has more of its assets invested in companies that score well according to the \\nSustainalytics ESG methodology.\\nPortfolio Controversy Score\\nThe Portfolio Controversy Score is the asset-weighted average level of the seriousness of the \\ncontroversial incidents related to companies in a fund’s portfolio. A low score is better than a high \\nscore, as it indicates the absence of controversies.\\nCompany-level controversial incident indicators from Sustainalytics reflect the most severe \\nESG-related controversy in which a firm is involved, classified as Low, Moderate, Significant,  \\nHigh, and Severe.\\nPortfolio Sustainability Score \\nThe Portfolio Sustainability Score is the Portfolio ESG Score minus the Portfolio  \\nControversy Score:\\nPortfolio Sustainability Score  =  Portfolio ESG Score  –  Portfolio Controversy Score/ 5\\nThe resulting score is displayed as a number between 0 and 100. From a sustainability  \\nstandpoint, a high score is better than a low score. A higher score indicates that a fund has,  \\non average, more of its assets invested in companies that score well according to the  \\nSustainalytics ESG methodology.'), Document(metadata={'page': 10, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook10Key Definitions\\n \\nPortfolio ESG Score\\nThe Portfolio ESG Score is an asset-weighted average of normalized company-level ESG  \\nScores for the covered holdings in the portfolio. Company-level ESG Scores from Sustainalytics \\nreflect companies’ management systems, practices, policies, and other indicators related to \\nenvironmental, social, and governance issues. Their company-level ESG scores use a 0-100  scale. \\nA high Portfolio ESG Score is better than a low score. At the portfolio level, high scores indicate \\nthat a fund has more of its assets invested in companies that score well according to the \\nSustainalytics ESG methodology.\\nPortfolio Controversy Score\\nThe Portfolio Controversy Score is the asset-weighted average level of the seriousness of the \\ncontroversial incidents related to companies in a fund’s portfolio. A low score is better than a high \\nscore, as it indicates the absence of controversies.\\nCompany-level controversial incident indicators from Sustainalytics reflect the most severe \\nESG-related controversy in which a firm is involved, classified as Low, Moderate, Significant,  \\nHigh, and Severe.\\nPortfolio Sustainability Score \\nThe Portfolio Sustainability Score is the Portfolio ESG Score minus the Portfolio  \\nControversy Score:\\nPortfolio Sustainability Score  =  Portfolio ESG Score  –  Portfolio Controversy Score/ 5\\nThe resulting score is displayed as a number between 0 and 100. From a sustainability  \\nstandpoint, a high score is better than a low score. A higher score indicates that a fund has,  \\non average, more of its assets invested in companies that score well according to the  \\nSustainalytics ESG methodology.')]\n",
            "[Document(metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.'), Document(metadata={'page': 5, 'source': 'https://s21.q4cdn.com/198919461/files/doc_downloads/press_kits/2016/Morningstar-Sustainable-Investing-Handbook.pdf'}, page_content='The Morningstar Sustainable Investing Handbook5Using the Morningstar Sustainability Rating for funds \\nOur Sustainability Rating allows investors to assess how well the companies in a fund’s  \\nportfolio are managing their ESG risks and opportunities. The rating will also allow investors to \\ncompare funds across categories and relative to benchmarks using specific ESG factors. \\nThe ratings can serve as an initial screen for investors interested in sustainability and ESG  \\nfactors.  They are also a useful starting point for investors wanting to know more about a manager’s \\ninvestment process and how it relates to sustainable investing.\\nThe ratings will help investors determine both the level of sustainability in their existing portfolios \\nand allow them to set sustainability targets. Some investors, for example, may prefer only  \\nfunds that have high ratings; others may wish to avoid funds with low ratings. Still others may \\nwish to achieve an above-average rating across all funds in their portfolios.\\nIt is important to note that the ratings are portfolio-based, not performance-based. They do not \\nreflect a fund’s performance on either an absolute or risk-adjusted basis, nor are they a  \\nqualitative Morningstar evaluation of a fund’s merits. They should not be the sole basis for an \\ninvestment decision.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator document_relevance_grader> on run eab568da-606d-4d0d-a9ba-94744d0989d4: ResourceExhausted('Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\", line 1181, in __call__\n",
            "    return _end_unary_response_blocking(state, call, False, None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\", line 1006, in _end_unary_response_blocking\n",
            "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
            "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
            "\tstatus = StatusCode.RESOURCE_EXHAUSTED\n",
            "\tdetails = \"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\"\n",
            "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.253.63.95:443 {created_time:\"2024-07-25T03:40:20.217027098+00:00\", grpc_status:8, grpc_message:\"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\"}\"\n",
            ">\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 1258, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 582, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/run_helpers.py\", line 579, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"<ipython-input-58-1d4a0cd4ea09>\", line 39, in document_relevance_grader\n",
            "    score = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 2875, in invoke\n",
            "    input = step.invoke(input, config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 5057, in invoke\n",
            "    return self.bound.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 270, in invoke\n",
            "    self.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 703, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 560, in generate\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 550, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 775, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 1134, in _generate\n",
            "    return self._generate_gemini(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 1279, in _generate_gemini\n",
            "    response = _completion_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 587, in _completion_with_retry\n",
            "    return _completion_with_retry_inner(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 336, in wrapped_f\n",
            "    return copy(f, *args, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 475, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 418, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 185, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 478, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_vertexai/chat_models.py\", line 580, in _completion_with_retry_inner\n",
            "    return generation_method(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/client.py\", line 2287, in generate_content\n",
            "    response = rpc(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n",
            "    raise exceptions.from_grpc_error(exc) from exc\n",
            "google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n"
          ]
        }
      ],
      "source": [
        "from langsmith.schemas import Example, Run\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "def document_relevance_grader(root_run: Run, example: Example) -> dict:\n",
        "    \"\"\"\n",
        "    A simple evaluator that checks to see if retrieved documents are relevant to the question\n",
        "    \"\"\"\n",
        "\n",
        "    # Get documents and question\n",
        "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"get_answer\")\n",
        "    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieve_docs\")\n",
        "    doc_txt = \"\\n\\n\".join(doc.page_content for doc in retrieve_run.outputs[\"output\"])\n",
        "    question = retrieve_run.inputs[\"question\"]\n",
        "\n",
        "    # Data model for grade\n",
        "    class GradeDocuments(BaseModel):\n",
        "        \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "        binary_score: int = Field(description=\"Documents are relevant to the question, 1 or 0\")\n",
        "\n",
        "    # LLM with function call\n",
        "    llm = ChatVertexAI(model=\"gemini-1.5-pro-001\")\n",
        "    structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "    # Prompt\n",
        "    system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "        Give a binary score 1 or 0 score, where 1 means that the document is relevant to the question.\"\"\"\n",
        "\n",
        "    grade_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    retrieval_grader = grade_prompt | structured_llm_grader\n",
        "    score = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
        "    return {\"key\": \"document_relevance\", \"score\": int(score.binary_score)}\n",
        "\n",
        "def answer_hallucination_grader(root_run: Run, example: Example) -> dict:\n",
        "    \"\"\"\n",
        "    A simple evaluator that checks to see the answer is grounded in the documents\n",
        "    \"\"\"\n",
        "\n",
        "    # Get documents and answer\n",
        "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"get_answer\")\n",
        "    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieve_docs\")\n",
        "    doc_txt = \"\\n\\n\".join(doc.page_content for doc in retrieve_run.outputs[\"output\"])\n",
        "    generation = rag_pipeline_run.outputs[\"answer\"]\n",
        "\n",
        "    # Data model\n",
        "    class GradeHallucinations(BaseModel):\n",
        "        \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "        binary_score: int = Field(description=\"Answer is grounded in the facts, 1 or 0\")\n",
        "\n",
        "    # LLM with function call\n",
        "    llm = ChatVertexAI(model=\"gemini-1.5-pro-001\")\n",
        "    structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "    # Prompt\n",
        "    system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "         Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "    hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "    score = hallucination_grader.invoke({\"documents\": doc_txt, \"generation\": generation})\n",
        "    return {\"key\": \"answer_hallucination\", \"score\": int(score.binary_score)}\n",
        "\n",
        "from langsmith.evaluation import evaluate\n",
        "experiment_results = evaluate(\n",
        "    predict_rag_answer,\n",
        "    data=dataset_name,\n",
        "    evaluators=[document_relevance_grader,answer_hallucination_grader],\n",
        "    experiment_prefix= \"LCEL context, gpt-3.5-turbo\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "704c71b4-723d-42ac-b87e-4ea747673370",
      "metadata": {
        "id": "704c71b4-723d-42ac-b87e-4ea747673370",
        "outputId": "5b5ab7f2-2dd8-4951-f536-e0a5f2b8d0b3",
        "colab": {
          "referenced_widgets": [
            "8e999731fd084a37a1c76cdfb2eab8dd"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'LCEL context, gpt-3.5-turbo-b7baef73' at:\n",
            "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/e5197c9e-24ab-405a-82c5-cef7afadb1b4/compare?selectedSessions=4a7d10a7-e26e-4906-ba79-b03fc2ca31ce\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e999731fd084a37a1c76cdfb2eab8dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "702960a3-353a-43e9-9225-9f7df0a5b3a0",
      "metadata": {
        "id": "702960a3-353a-43e9-9225-9f7df0a5b3a0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb98fccd-a4f6-4795-805e-a06f189fa4d5",
      "metadata": {
        "id": "fb98fccd-a4f6-4795-805e-a06f189fa4d5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8734055a-b1c9-48ed-b28e-52f99e376b83",
      "metadata": {
        "id": "8734055a-b1c9-48ed-b28e-52f99e376b83"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff477fa-f7d0-42b1-9ce1-e8b9b0f3ccc2",
      "metadata": {
        "id": "bff477fa-f7d0-42b1-9ce1-e8b9b0f3ccc2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcba540e-b653-4af9-9ef2-3ade7914ebbd",
      "metadata": {
        "id": "dcba540e-b653-4af9-9ef2-3ade7914ebbd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c0c0410bf8645beb7b83e703dc8c861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_123d11fd237f4c41ab6ca993edad0a13",
              "IPY_MODEL_eca1f39306e94fc8a7ea11c673eedd86",
              "IPY_MODEL_f77682e0767b4167b00966a674a77b07"
            ],
            "layout": "IPY_MODEL_ce1ee936cc084ffb93afc5fc8b45e71d"
          }
        },
        "123d11fd237f4c41ab6ca993edad0a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e134399c7743cda47a54c268a50385",
            "placeholder": "​",
            "style": "IPY_MODEL_13cf91d3075f4cac8b2d7ac2f359681a",
            "value": ""
          }
        },
        "eca1f39306e94fc8a7ea11c673eedd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98d3d8612b4e45fc84e3e7e3258f5fb7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5095e4d3ace243168737a4841d17e377",
            "value": 1
          }
        },
        "f77682e0767b4167b00966a674a77b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74862c20eaf14739a72747bd98f2d844",
            "placeholder": "​",
            "style": "IPY_MODEL_d2270e44610641988864b3767276acb5",
            "value": " 3/? [00:24&lt;00:00,  7.01s/it]"
          }
        },
        "ce1ee936cc084ffb93afc5fc8b45e71d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e134399c7743cda47a54c268a50385": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13cf91d3075f4cac8b2d7ac2f359681a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98d3d8612b4e45fc84e3e7e3258f5fb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5095e4d3ace243168737a4841d17e377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74862c20eaf14739a72747bd98f2d844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2270e44610641988864b3767276acb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e03451ccafd47b49dbb84af893331bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_414240c21af14133a5dbc9426f99ce11",
              "IPY_MODEL_1a233e2eaa6948e4b0f1b2539e5ea8a4",
              "IPY_MODEL_5c43d666124b4d7eb2204e240bbb730a"
            ],
            "layout": "IPY_MODEL_f5a893eec5b04076b44ab224c78b36d2"
          }
        },
        "414240c21af14133a5dbc9426f99ce11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f4b98f7ddd948b0a15547eadf38796e",
            "placeholder": "​",
            "style": "IPY_MODEL_d362a8385d7445569ee43390c6b84fda",
            "value": ""
          }
        },
        "1a233e2eaa6948e4b0f1b2539e5ea8a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a80f18b8e2140d3af555eec35db9018",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e60f177ac50e47c6a59e9e11d5fa0af3",
            "value": 1
          }
        },
        "5c43d666124b4d7eb2204e240bbb730a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3469a9fe33d24624a354938a9cdb5b8a",
            "placeholder": "​",
            "style": "IPY_MODEL_0904bb33ad2b4e11ae2db0130fe1417f",
            "value": " 3/? [00:33&lt;00:00,  8.75s/it]"
          }
        },
        "f5a893eec5b04076b44ab224c78b36d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f4b98f7ddd948b0a15547eadf38796e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d362a8385d7445569ee43390c6b84fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a80f18b8e2140d3af555eec35db9018": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e60f177ac50e47c6a59e9e11d5fa0af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3469a9fe33d24624a354938a9cdb5b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0904bb33ad2b4e11ae2db0130fe1417f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ef3f47300c5421d84e113bf7f0af907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02bfb7f629f84acab0cf73f8880e8ff7",
              "IPY_MODEL_29432ed5b90544f9b1627fc2f8568c09",
              "IPY_MODEL_6cdf8546db524c349a7faa54f6addcf4"
            ],
            "layout": "IPY_MODEL_6dbe654ec2b64118bcf8ca3f6fb7888e"
          }
        },
        "02bfb7f629f84acab0cf73f8880e8ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_118d8c39c56f4a8990ad41fbc2311c50",
            "placeholder": "​",
            "style": "IPY_MODEL_544266fa880546bd8f5481aa3d013b6d",
            "value": ""
          }
        },
        "29432ed5b90544f9b1627fc2f8568c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_499889b7cc5e4188ac6e27ea3919cda1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a56d3f9a6a5a4877a358ea06ad8873f9",
            "value": 1
          }
        },
        "6cdf8546db524c349a7faa54f6addcf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2da99df97c140ba936e1bd42b7f9d1a",
            "placeholder": "​",
            "style": "IPY_MODEL_df8b3d0dbccf4fd1b477c33c65c88c48",
            "value": " 3/? [01:01&lt;00:00, 16.71s/it]"
          }
        },
        "6dbe654ec2b64118bcf8ca3f6fb7888e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118d8c39c56f4a8990ad41fbc2311c50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "544266fa880546bd8f5481aa3d013b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "499889b7cc5e4188ac6e27ea3919cda1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a56d3f9a6a5a4877a358ea06ad8873f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2da99df97c140ba936e1bd42b7f9d1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df8b3d0dbccf4fd1b477c33c65c88c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66c7bcc923724d61b53616aebca12e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e72b54fdca494e5eac21c3b00699a55c",
              "IPY_MODEL_0a129fe982da4cba9ca188336ef1e5df",
              "IPY_MODEL_ca6c2501bc9049f69c136153dc7e932f"
            ],
            "layout": "IPY_MODEL_ea7ad3cfa498490d8a4179edd0da8abd"
          }
        },
        "e72b54fdca494e5eac21c3b00699a55c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a2ef2d99be44562b7daafdde085e07a",
            "placeholder": "​",
            "style": "IPY_MODEL_8acb1f2ec4354deca2a277369a57a957",
            "value": ""
          }
        },
        "0a129fe982da4cba9ca188336ef1e5df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4c889b70ef644648b4c66fb0b5c8512",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b0e854a4b68419ba7bb3868bb67756a",
            "value": 1
          }
        },
        "ca6c2501bc9049f69c136153dc7e932f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_633268d9bc164c059f335cbf78ea3193",
            "placeholder": "​",
            "style": "IPY_MODEL_cf20a0b5033f443e8916eac14115dddc",
            "value": " 3/? [00:53&lt;00:00, 15.67s/it]"
          }
        },
        "ea7ad3cfa498490d8a4179edd0da8abd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a2ef2d99be44562b7daafdde085e07a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8acb1f2ec4354deca2a277369a57a957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4c889b70ef644648b4c66fb0b5c8512": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4b0e854a4b68419ba7bb3868bb67756a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "633268d9bc164c059f335cbf78ea3193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf20a0b5033f443e8916eac14115dddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}