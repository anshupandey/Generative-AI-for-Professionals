{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-for-Professionals/blob/main/OpenAI_External_API_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KQD1SXcnSYu",
        "outputId": "6d077b44-7f71-4229-c03f-283c6a5eab70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai --quiet\n",
        "!pip install ipython --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call one or many functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code."
      ],
      "metadata": {
        "id": "CMs1SliPer2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supported models\n",
        "Not all model versions are trained with function calling data. Function calling is supported with the following models: gpt-4, gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4-0613, gpt-3.5-turbo, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, and gpt-3.5-turbo-0613\n",
        "\n",
        "In addition, parallel function calls is supported on the following models: gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-3.5-turbo-0125, and gpt-3.5-turbo-1106"
      ],
      "metadata": {
        "id": "O6voT6dEZOEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from openai import OpenAI\n",
        "os.environ['OPENAI_API_KEY'] = \"sk-xxxxxxxxxxxxxxxxxxx\"\n",
        "\n",
        "# Initializing a client object: gets the API Key from environment variable OPENAI_API_KEY\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "qP21QdTRno_A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "owm_api = \"xxxxxxxxxxx\"\n",
        "\n",
        "# create a dummy function to respond temperature\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "  \"This function can be used to fetch the current weather information when provided with details such as location and unit for temperature\"\n",
        "\n",
        "  url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={owm_api}\"\n",
        "  response = requests.get(url)\n",
        "  temp = response.json()['main']['temp']\n",
        "  forecast = [response.json()['weather'][0]['main'],response.json()['weather'][0]['description']]\n",
        "\n",
        "  weather_info = {\n",
        "      \"location\":location,\n",
        "      \"temperature\":temp,\n",
        "      \"unit\":'Kelvin',\n",
        "      \"forecast\":forecast\n",
        "  }\n",
        "  return json.dumps(weather_info)"
      ],
      "metadata": {
        "id": "FUAVQTH1FCob"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_weather(\"Mumbai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zvpewsATGB30",
        "outputId": "0cbfc1a7-6f12-47a0-aa71-019ed866cdcc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"location\": \"Mumbai\", \"temperature\": 296.14, \"unit\": \"Kelvin\", \"forecast\": [\"Smoke\", \"smoke\"]}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [{\n",
        "                            \"type\":\"function\",\n",
        "                            \"function\":{\"name\": \"get_current_weather\",\n",
        "                           \"description\": \"This function can be used to fetch the current weather information when provided with details such as location and unit for temperature\",\n",
        "                           \"parameters\": {\n",
        "                               \"type\": \"object\",\n",
        "                               \"properties\": {\"location\": {\n",
        "                                   \"type\": \"string\",\n",
        "                                   \"description\": \"The name of city/state/country for which weather information is to be fetched\",},},\n",
        "                               \"required\": [\"location\",],},},}]\n",
        "\n",
        "\n",
        "\n",
        "def get_response(messages, tools,model=\"gpt-3.5-turbo-1106\"):\n",
        "  response = client.chat.completions.create(model = model, messages = messages, tools=tools, tool_choice='auto', temperature=0.5,)\n",
        "  response = response.choices[0].message\n",
        "  tool_calls = response.tool_calls\n",
        "\n",
        "  try:\n",
        "    if tool_calls:\n",
        "      print(\"Making a function call\")\n",
        "      # get available functions\n",
        "      available_functions = {\n",
        "          \"get_current_weather\":get_current_weather,\n",
        "      }\n",
        "\n",
        "      # get details to call the function assuming there is only one function call\n",
        "      func_name = tool_calls[0].function.name\n",
        "      func_to_call = available_functions[func_name]\n",
        "      func_args = json.loads(tool_calls[0].function.arguments)\n",
        "\n",
        "      # call the external api by calling the function\n",
        "      func_response = func_to_call(**func_args)\n",
        "\n",
        "      # again make a call to openai api to communicate results from external function/API\n",
        "      messages.append(response)\n",
        "      messages.append(\n",
        "          {\"tool_call_id\":tool_calls[0].id,\n",
        "          \"role\":\"tool\",\n",
        "          \"name\":func_name,\n",
        "          \"content\":str(func_response)}\n",
        "      )\n",
        "\n",
        "      second_response = client.chat.completions.create(\n",
        "      model = \"gpt-3.5-turbo-1106\",\n",
        "      messages = messages)\n",
        "      return second_response\n",
        "    else:\n",
        "      return response.content\n",
        "  except Exception as e:\n",
        "      print(\"Error occurred \",e)\n",
        "      return response\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9gdvGUJGbheA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"Provide a 2 line explanation for AI\"}]\n",
        "response = get_response(messages, tools)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWL0wVqFdeBm",
        "outputId": "4ec4827f-6563-4d21-c1f2-7a1d6969ee41"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI, or Artificial Intelligence, is a branch of computer science that aims to create intelligent machines capable of simulating human cognitive processes. It involves the development of algorithms and systems that can perceive, reason, learn, and act in a way that mimics human intelligence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"How is the weather in Mumbai today?\"}]\n",
        "response = get_response(messages, tools)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0H4DaivdmPW",
        "outputId": "1ed3dbe2-8da1-427e-c81a-ac3cf2dae0e3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making a function call\n",
            "The weather in Mumbai today is smoky with a temperature of 297.14 Kelvin.\n"
          ]
        }
      ]
    }
  ]
}